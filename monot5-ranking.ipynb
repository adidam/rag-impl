{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chunk and Index data into DB ##","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nretrieval_model = \"BAAI/LLM-Embedder\"\n# retrieval_model = \"sentence-transformers/all-miniLM-L6-v2\"\n\nTOKEN_LIMIT = 512\nSLIDING_WINDOW_OVERLAP = 100\n\n# Function for chunking with token limit and sliding window\ndef chunk_with_token_limit(text, token_limit, overlap):\n    sentences = sent_tokenize(text)  # Split text into sentences\n    chunks = []  # Store resulting chunks\n    current_chunk = []  # Temporarily hold sentences for the current chunk\n    current_chunk_tokens = 0  # Token count for the current chunk\n\n    for sentence in sentences:\n        # Tokenize the sentence and calculate its token count\n        sentence_tokens = tokenizer.tokenize(sentence)\n        num_tokens = len(sentence_tokens)\n\n        # print(f\"Tokens: {sentence_tokens[0]}\")\n\n        # If adding this sentence exceeds the token limit\n        if current_chunk_tokens + num_tokens > token_limit:\n            # Save the current chunk\n            chunk_text = \" \".join(current_chunk)\n            chunks.append(chunk_text)\n\n            # Prepare the next chunk with overlap\n            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n\n        # Add the sentence to the current chunk\n        current_chunk.append(sentence)\n        current_chunk_tokens += num_tokens\n\n    # Add the last chunk if it exists\n    if current_chunk:\n        chunk_text = \" \".join(current_chunk)\n        chunks.append(chunk_text)\n\n    return chunks\n\ndef process_document_with_identifiers(document):\n    processed_data = []\n    title_count = -1  # to start from 0\n    print(\"document>>>>>>>\",document)\n    for section in document:\n        section_chunks = []\n        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n        title_count += 1  # Increment title count\n\n        # Tokenize the section into sentences\n        sentences = sent_tokenize(section)\n        for sentence in sentences:\n            if sentence.startswith(\"Title:\"):\n                # New document detected\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n                passage_count = [ord('a')]  # Reset passage count for the new document\n            else:\n                # Sentence under the current document\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                #print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n\n                # Increment passage_count intelligently\n                i = len(passage_count) - 1\n                while i >= 0:\n                    passage_count[i] += 1\n                    if passage_count[i] > ord('z'):\n                        passage_count[i] = ord('a')\n                        if i == 0:\n                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n                        i -= 1\n                    else:\n                        break\n\n\n        print(\"section_chunks>>>>>>>\",section_chunks)\n        processed_data.append(section_chunks)\n\n    return processed_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Check uniqueness of data before insertion** ##","metadata":{}},{"cell_type":"code","source":"import hashlib\n\n# Function to generate a hash based on content and key metadata\ndef generate_hash(content, metadata):\n    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes(collection):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n    existing_hashes = set()\n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes_milvus(all_records):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    existing_hashes = set()\n    print(f\"all records >>> {len(all_records)}\")    \n    if all_records == None or len(all_records) == 0:\n        return existing_hashes\n        \n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadata\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Store and retrieve data from Milvus** ##","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom pymilvus import connections\nfrom pymilvus import FieldSchema, CollectionSchema, DataType, Collection\nfrom pymilvus import MilvusClient\nfrom pymilvus import utility\n\nclass VectorDataStore:\n    db_url = \"http://localhost:19530\"\n    #description = f\"collection created for {self.name}\"\n\n    def __init__(self, path=\"/content/ragbench.db\"):\n        self.client = MilvusClient(path)\n\n\n\n    def create_collection(self, name, vec_dim=128):\n        if self.client.has_collection(name):\n            self.default_collection_name = name\n\n        self.description = f\"collection to store {name}\"\n\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embedding\",\n            index_type=\"HNSW\",\n            params = {\n                \"M\": 16, # Number of bidirectional links created for each element\n                \"efConstruction\": 200 # Size of the dynamic list for the nearest neighbours during indexing\n            }\n            metric_type=\"COSINE\"\n        )\n        schema = self.client.create_schema(\n            auto_id=False,\n            enable_dynamic_fields=True,\n        )\n        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n        schema.add_field(field_name=\"timestamp\", datatype=DataType.INT64)\n        \n        collection = self.client.create_collection(collection_name=name,\n                                       schema=schema,\n                                       index_params=index_params)\n        self.current_collection = collection\n        return collection\n\n\n    def get_collection(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exist.\")\n        self.current_collection = Collection(name)\n        return self.current_collection\n\n    def get_all_records(self, collection):\n        all_records = self.client.query(\n            collection_name=collection,\n            filter=None,\n            output_fields=[\"documents\", \"metadata\"],\n            limit=10000\n        )\n        if all_records == None:\n            all_records = []\n\n        return all_records\n\n    def has_entities(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exists.\")\n        self.default_collection = name\n        collection_stats = self.client.get_collection_stats(collection_name)\n        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n        return count\n\n    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n                documents: list[str], embeddings: np.ndarray, ids: list[int]):\n\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n\n        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n\n        data = []\n        for meta, doc, emb, id in zip(metadata, documents, embeddings, ids):\n          datum = {\n              \"pk\": id,\n              \"metadata\": meta,\n              \"documents\": doc,\n              \"embedding\": emb.tolist(),\n              \"timestamp\": int(time.time()),\n          }\n          data.append(datum)\n\n        self.client.insert(collection_name, data)\n        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n\n    def drop_collection(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.drop_collection(collection_name)\n        print(f\"Dropped collection '{collection_name}'.\")\n\n    def delete_all(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.delete(collection_name, expr=\"pk >= 0\")\n        self.client.flush([collection_name])\n\n    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> list[dict[str, any]]:\n        \"\"\"\n        Search across all collections for the top-k closest embeddings.\n        :param query_embedding: The embedding vector to search for.\n        :param top_k: Number of top results to retrieve.\n        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n        \"\"\"\n        results = []\n        #collections = self.client.list_collections()\n        collections = [\"ragbench_collection_techqa_v09\"]\n        start_time = time.time()\n        for collection_name in collections:\n            if not self.client.has_collection(collection_name):\n                continue\n\n            # Set params to COSINE to match chromadb\n            search_params = {\n                \"metric_type\": \"COSINE\", \n                \"params\": {\n                    \"ef\": 64\n                }\n            }\n\n            search_results = self.client.search(\n                collection_name=collection_name,\n                data=[query_embedding],\n                anns_field=\"embedding\",\n                search_params=search_params,\n                limit=top_k,\n                output_fields=[\"metadata\", \"documents\"]\n            )\n\n            for hits in search_results:\n                for hit in hits:\n                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n                    results.append({\n                        \"collection\": collection_name,\n                        \"id\": hit[\"id\"],\n                        \"metadata\": hit[\"entity\"][\"metadata\"],\n                        \"distance\": hit[\"distance\"],\n                        \"documents\": hit[\"entity\"][\"documents\"]\n                      })\n\n        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n        end_time = time.time()\n        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n        return results\n\n    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n      \"\"\"\n      Extract embedding values from search results.\n      :param search_results: List of dictionaries containing search results.\n      :return: List of embedding vectors as NumPy arrays.\n      \"\"\"\n      return [np.array(result[\"documents\"]) for result in search_results if \"documents\" in result]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nall_documents = []\nall_ids = []\nall_metadatas = []\n\n# Process each dataset\ndoc_idx = 0  # Global document index for unique IDs\nfor dataset in datasets:\n    data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n    # #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n    # data = data.select(range(2))\n    for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n        # Extract document text\n        doc_text = row.get('documents', '')\n\n        # Skip if no documents found\n        if not doc_text:\n            continue\n\n        # Process the document\n        processed_output = process_document_with_identifiers(doc_text)\n        added_item_idxs = set()\n\n        # Populate the lists\n        for section_idx, section in enumerate(processed_output):\n            for item_idx, (prefix, content) in enumerate(section):\n                # Skip if this item_idx has already been processed\n                if item_idx in added_item_idxs:\n                    continue\n\n                # Add the item_idx to the set to track it\n                added_item_idxs.add(item_idx)\n\n                # Add the document\n                document = f\"[{prefix}] {content}\"\n                all_documents.append(document)\n\n                # Construct a globally unique ID\n                doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n                all_ids.append(doc_id)\n\n                # Construct metadata\n                metadata = {\n                    \"dataset\": dataset,\n                    \"global_index\": doc_idx,\n                    \"section_index\": section_idx,\n                    \"item_index\": item_idx,\n                    \"prefix\": prefix,\n                    \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n                }\n                all_metadatas.append(metadata)\n\n        doc_idx += 1  # Increment global document index\n\n# Step 4: Generate Embeddings\n#embedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nembedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nbatch_size = 2500  # Adjust based on available memory\n\n# Generate embeddings in batches\nall_embeddings = []\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n    batch_docs = all_documents[i:i + batch_size]\n    batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n    all_embeddings.extend(batch_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions = ['When was Rolex founded?', 'How large is the region of Macedonia?', \n             'Where is GMT Games headquartered?', 'What state is directly north of North Carolina?', \n             'When was Brown v. Board of Education?',\n             \n             'What star of Parks and Recreation appeared in November?', \n             'What is the capacity of the Stadium, other than Kauffman Stadium, designed by Charles Deaton ?', \n             'What was the island, on which Marinelli Glacier is located, formerly known as?', \n             'The American Sweetgum is the hostplant of what kind of bug?', \n             'The name of the Japanese rock band T-Bolan was inspired by the name of an English rock band formed in what year?',\n             \n             'symptoms of pregnancy before a missed period', 'monoclonal antibodies biology definition', \n             'what is iron sulfate', \"who sang one day i'll fly away\", \n             'describe the antebellum reform movement period'\n            ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Retrieve Candidates from DB **##","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}