{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:18:02.451058Z","iopub.execute_input":"2025-01-01T20:18:02.451283Z","iopub.status.idle":"2025-01-01T20:18:07.374246Z","shell.execute_reply.started":"2025-01-01T20:18:02.451259Z","shell.execute_reply":"2025-01-01T20:18:07.373180Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\n\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:18:36.909924Z","iopub.execute_input":"2025-01-01T20:18:36.910233Z","iopub.status.idle":"2025-01-01T20:18:38.172452Z","shell.execute_reply.started":"2025-01-01T20:18:36.910210Z","shell.execute_reply":"2025-01-01T20:18:38.171735Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Chunk and Index data into DB ##","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom transformers import AutoTokenizer\n\nfrom datasets import load_dataset\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nretrieval_model = \"BAAI/LLM-Embedder\"\n# retrieval_model = \"sentence-transformers/all-miniLM-L6-v2\"\n\ntokenizer = AutoTokenizer.from_pretrained(retrieval_model)\n\nTOKEN_LIMIT = 512\nSLIDING_WINDOW_OVERLAP = 100\n\n# Function for chunking with token limit and sliding window\ndef chunk_with_token_limit(text, token_limit, overlap):\n    sentences = sent_tokenize(text)  # Split text into sentences\n    chunks = []  # Store resulting chunks\n    current_chunk = []  # Temporarily hold sentences for the current chunk\n    current_chunk_tokens = 0  # Token count for the current chunk\n\n    for sentence in sentences:\n        # Tokenize the sentence and calculate its token count\n        sentence_tokens = tokenizer.tokenize(sentence)\n        num_tokens = len(sentence_tokens)\n\n        # print(f\"Tokens: {sentence_tokens[0]}\")\n\n        # If adding this sentence exceeds the token limit\n        if current_chunk_tokens + num_tokens > token_limit:\n            # Save the current chunk\n            chunk_text = \" \".join(current_chunk)\n            chunks.append(chunk_text)\n\n            # Prepare the next chunk with overlap\n            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n\n        # Add the sentence to the current chunk\n        current_chunk.append(sentence)\n        current_chunk_tokens += num_tokens\n\n    # Add the last chunk if it exists\n    if current_chunk:\n        chunk_text = \" \".join(current_chunk)\n        chunks.append(chunk_text)\n\n    return chunks\n\ndef process_document_with_identifiers(document):\n    processed_data = []\n    title_count = -1  # to start from 0\n    # print(\"document>>>>>>>\",document)\n    for section in document:\n        section_chunks = []\n        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n        title_count += 1  # Increment title count\n\n        # Tokenize the section into sentences\n        sentences = sent_tokenize(section)\n        for sentence in sentences:\n            if sentence.startswith(\"Title:\"):\n                # New document detected\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n                passage_count = [ord('a')]  # Reset passage count for the new document\n            else:\n                # Sentence under the current document\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                #print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n\n                # Increment passage_count intelligently\n                i = len(passage_count) - 1\n                while i >= 0:\n                    passage_count[i] += 1\n                    if passage_count[i] > ord('z'):\n                        passage_count[i] = ord('a')\n                        if i == 0:\n                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n                        i -= 1\n                    else:\n                        break\n\n\n        # print(\"section_chunks>>>>>>>\",section_chunks)\n        processed_data.append(section_chunks)\n\n    return processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:18:45.159471Z","iopub.execute_input":"2025-01-01T20:18:45.159910Z","iopub.status.idle":"2025-01-01T20:18:51.986397Z","shell.execute_reply.started":"2025-01-01T20:18:45.159882Z","shell.execute_reply":"2025-01-01T20:18:51.985528Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d7e6233c746479cacbb41d256111081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff5133385124854a16a199448c18730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2840ddda674e4859add53670dfff61f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76e42de16ff44a687a423ebb9c5ebd1"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## **Check uniqueness of data before insertion** ##","metadata":{}},{"cell_type":"code","source":"import hashlib\n\n# Function to generate a hash based on content and key metadata\ndef generate_hash(content, metadata):\n    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes(collection):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n    existing_hashes = set()\n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes_milvus(all_records):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    existing_hashes = set()\n    print(f\"all records >>> {len(all_records)}\")    \n    if all_records == None or len(all_records) == 0:\n        return existing_hashes\n        \n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadata\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:05:51.269910Z","iopub.execute_input":"2025-01-01T20:05:51.270193Z","iopub.status.idle":"2025-01-01T20:05:51.276138Z","shell.execute_reply.started":"2025-01-01T20:05:51.270172Z","shell.execute_reply":"2025-01-01T20:05:51.275273Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Store and retrieve data from Milvus** ##","metadata":{}},{"cell_type":"code","source":"!pip install pymilvus pymilvus[model]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:19:18.346136Z","iopub.execute_input":"2025-01-01T20:19:18.346642Z","iopub.status.idle":"2025-01-01T20:19:25.276042Z","shell.execute_reply.started":"2025-01-01T20:19:18.346612Z","shell.execute_reply":"2025-01-01T20:19:25.275082Z"}},"outputs":[{"name":"stdout","text":"Collecting pymilvus\n  Downloading pymilvus-2.5.2-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (71.0.4)\nRequirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.64.1)\nRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\nCollecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\nRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.1.4)\nCollecting milvus-lite>=2.4.0 (from pymilvus)\n  Downloading milvus_lite-2.4.11-py3-none-manylinux2014_x86_64.whl.metadata (9.2 kB)\nCollecting milvus-model>=0.1.0 (from pymilvus[model])\n  Downloading milvus_model-0.2.11-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.66.5)\nRequirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.44.2)\nCollecting onnxruntime (from milvus-model>=0.1.0->pymilvus[model])\n  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.24.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.19.1)\nCollecting coloredlogs (from onnxruntime->milvus-model>=0.1.0->pymilvus[model])\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (24.3.25)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (4.12.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model])\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\nDownloading pymilvus-2.5.2-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading milvus_lite-2.4.11-py3-none-manylinux2014_x86_64.whl (45.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading milvus_model-0.2.11-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-dotenv, milvus-lite, humanfriendly, coloredlogs, pymilvus, onnxruntime, milvus-model\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 milvus-lite-2.4.11 milvus-model-0.2.11 onnxruntime-1.20.1 pymilvus-2.5.2 python-dotenv-1.0.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom pymilvus import connections\nfrom pymilvus import FieldSchema, CollectionSchema, DataType, Collection\nfrom pymilvus import MilvusClient\nfrom pymilvus import utility\n\nclass VectorDataStore:\n    db_url = \"http://localhost:19530\"\n    #description = f\"collection created for {self.name}\"\n\n    def __init__(self, path=\"/content/ragbench.db\"):\n        self.client = MilvusClient(path)\n\n\n\n    def create_collection(self, name, vec_dim=128):\n        if self.client.has_collection(name):\n            self.default_collection_name = name\n\n        self.description = f\"collection to store {name}\"\n\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embedding\",\n            index_type=\"HNSW\",\n            params={\n                \"M\": 16, # Number of bidirectional links created for each element\n                \"efConstruction\": 200 # Size of the dynamic list for the nearest neighbours during indexing\n            },\n            metric_type=\"COSINE\"\n        )\n        schema = self.client.create_schema(\n            auto_id=False,\n            enable_dynamic_fields=True,\n        )\n        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n        schema.add_field(field_name=\"timestamp\", datatype=DataType.INT64)\n        \n        collection = self.client.create_collection(collection_name=name,\n                                       schema=schema,\n                                       index_params=index_params)\n        self.current_collection = collection\n        return collection\n\n\n    def get_collection(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exist.\")\n        self.current_collection = Collection(name)\n        return self.current_collection\n\n    def get_all_records(self, collection):\n        all_records = self.client.query(\n            collection_name=collection,\n            filter=None,\n            output_fields=[\"documents\", \"metadata\"],\n            limit=10000\n        )\n        if all_records == None:\n            all_records = []\n\n        return all_records\n\n    def has_entities(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exists.\")\n        self.default_collection = name\n        collection_stats = self.client.get_collection_stats(collection_name)\n        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n        return count\n\n    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n                documents: list[str], embeddings: np.ndarray, ids: list[int]):\n\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n\n        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n\n        data = []\n        for meta, doc, emb, id in zip(metadata, documents, embeddings, ids):\n          datum = {\n              \"pk\": id,\n              \"metadata\": meta,\n              \"documents\": doc,\n              \"embedding\": emb.tolist(),\n              \"timestamp\": int(time.time()),\n          }\n          data.append(datum)\n\n        self.client.insert(collection_name, data)\n        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n\n    def drop_collection(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.drop_collection(collection_name)\n        print(f\"Dropped collection '{collection_name}'.\")\n\n    def delete_all(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.delete(collection_name, expr=\"pk >= 0\")\n        self.client.flush([collection_name])\n\n    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> list[dict[str, any]]:\n        \"\"\"\n        Search across all collections for the top-k closest embeddings.\n        :param query_embedding: The embedding vector to search for.\n        :param top_k: Number of top results to retrieve.\n        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n        \"\"\"\n        results = []\n        #collections = self.client.list_collections()\n        collections = [\"ragbench_collection_techqa_v09\"]\n        start_time = time.time()\n        for collection_name in collections:\n            if not self.client.has_collection(collection_name):\n                continue\n\n            # Set params to COSINE to match chromadb\n            search_params = {\n                \"metric_type\": \"COSINE\", \n                \"params\": {\n                    \"ef\": 64\n                }\n            }\n\n            search_results = self.client.search(\n                collection_name=collection_name,\n                data=[query_embedding],\n                anns_field=\"embedding\",\n                search_params=search_params,\n                limit=top_k,\n                output_fields=[\"metadata\", \"documents\"]\n            )\n\n            for hits in search_results:\n                for hit in hits:\n                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n                    results.append({\n                        \"collection\": collection_name,\n                        \"id\": hit[\"id\"],\n                        \"metadata\": hit[\"entity\"][\"metadata\"],\n                        \"distance\": hit[\"distance\"],\n                        \"documents\": hit[\"entity\"][\"documents\"]\n                      })\n\n        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n        end_time = time.time()\n        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n        return results\n\n    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n      \"\"\"\n      Extract embedding values from search results.\n      :param search_results: List of dictionaries containing search results.\n      :return: List of embedding vectors as NumPy arrays.\n      \"\"\"\n      return [np.array(result[\"documents\"]) for result in search_results if \"documents\" in result]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:19:30.784052Z","iopub.execute_input":"2025-01-01T20:19:30.784388Z","iopub.status.idle":"2025-01-01T20:19:31.265354Z","shell.execute_reply.started":"2025-01-01T20:19:30.784358Z","shell.execute_reply":"2025-01-01T20:19:31.264690Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nall_documents = []\nall_ids = []\nall_metadatas = []\n\n# Process each dataset\ndoc_idx = 0  # Global document index for unique IDs\nfor dataset in datasets:\n    data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n    # #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n    # data = data.select(range(2))\n    for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n        # Extract document text\n        doc_text = row.get('documents', '')\n\n        # Skip if no documents found\n        if not doc_text:\n            continue\n\n        # Process the document\n        processed_output = process_document_with_identifiers(doc_text)\n        added_item_idxs = set()\n\n        # Populate the lists\n        for section_idx, section in enumerate(processed_output):\n            for item_idx, (prefix, content) in enumerate(section):\n                # Skip if this item_idx has already been processed\n                if item_idx in added_item_idxs:\n                    continue\n\n                # Add the item_idx to the set to track it\n                added_item_idxs.add(item_idx)\n\n                # Add the document\n                document = f\"[{prefix}] {content}\"\n                all_documents.append(document)\n\n                # Construct a globally unique ID\n                doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n                all_ids.append(doc_id)\n\n                # Construct metadata\n                metadata = {\n                    \"dataset\": dataset,\n                    \"global_index\": doc_idx,\n                    \"section_index\": section_idx,\n                    \"item_index\": item_idx,\n                    \"prefix\": prefix,\n                    \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n                }\n                all_metadatas.append(metadata)\n\n        doc_idx += 1  # Increment global document index\n\n# Step 4: Generate Embeddings\n#embedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nembedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nbatch_size = 2500  # Adjust based on available memory\n\n# Generate embeddings in batches\nall_embeddings = []\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n    batch_docs = all_documents[i:i + batch_size]\n    batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n    all_embeddings.extend(batch_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:08:46.702833Z","iopub.execute_input":"2025-01-01T20:08:46.703151Z","iopub.status.idle":"2025-01-01T20:08:47.649830Z","shell.execute_reply.started":"2025-01-01T20:08:46.703131Z","shell.execute_reply":"2025-01-01T20:08:47.648292Z"}},"outputs":[{"name":"stderr","text":"Processing hagrid: 0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"document>>>>>>> ['Rolex SA () is a Swiss luxury watchmaker. Originally founded as \"Wilsdorf and Davis\" by Hans Wilsdorf and Alfred Davis in London in 1905, the company registered \"Rolex\" as the brand name of its watches in 1908. In 1919, the company moved its base of operations to Geneva, Switzerland in order to avoid heavy taxation from a recovering post-war Britain, and \"Montres Rolex S.A.\" was registered as the new company name by Hans Wilsdorf in Geneva in 1920. Since 1960, the company has been owned by the Hans Wilsdorf Foundation, a private family trust.']\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-d0e6ca8c69d2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Process the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mprocessed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_document_with_identifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0madded_item_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-2d65b2ae485e>\u001b[0m in \u001b[0;36mprocess_document_with_identifiers\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# Sentence under the current document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{title_count}{''.join(chr(c) for c in passage_count)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mchunked_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_with_token_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKEN_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSLIDING_WINDOW_OVERLAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0;31m#print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunked_texts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-2d65b2ae485e>\u001b[0m in \u001b[0;36mchunk_with_token_limit\u001b[0;34m(text, token_limit, overlap)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Tokenize the sentence and calculate its token count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msentence_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"datastor = VectorDataStore()\n\ninsert_data = False\nstore_client = \"Milvus\"\nnum_records = 0\n\nif datastor.client.has_collection(collection_name):\n  num_records = datastor.has_entities(collection_name)\n  if num_records == 0:\n    insert_data = True\nelse:\n  datastor.create_collection(collection_name, embedder.get_sentence_embedding_dimension())\n  insert_data = True\n\nprint(f\"count >>> {num_records} insert_data >>> {insert_data}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding data to Milvus with enhanced duplicate check\nif not insert_data:\n    existing_hashes = set()\nelse:\n    existing_hashes = get_existing_hashes_milvus(datastor.get_all_records(collection_name))\n\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to Milvus\"):\n    batch_embeddings = all_embeddings[i:i + batch_size]\n    batch_metadatas = all_metadatas[i:i + batch_size]\n    batch_documents = all_documents[i:i + batch_size]\n    batch_ids = []\n\n    # Generate hashes for each document in the batch\n    for doc, metadata in zip(batch_documents, batch_metadatas):\n        doc_hash = generate_hash(doc, metadata)\n        if doc_hash not in existing_hashes:\n            batch_ids.append(doc_hash)\n            existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n        else:\n            print(f\"Skipping duplicate document: {doc[:50]}...\")  # Print a preview of the duplicate doc\n\n    # Add non-duplicate documents to the database\n    if batch_ids:  # Ensure there are non-duplicate documents to add\n        # Add the batch to the Milvus collection\n        if store_client == \"Milvus\" and insert_data:\n            datastor.insert(collection_name,\n                metadata=batch_metadatas,\n                documents=batch_documents,\n                embeddings=np.array(batch_embeddings),\n                ids=batch_ids\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions = ['When was Rolex founded?', 'How large is the region of Macedonia?', \n             'Where is GMT Games headquartered?', 'What state is directly north of North Carolina?', \n             'When was Brown v. Board of Education?',\n             \n             'What star of Parks and Recreation appeared in November?', \n             'What is the capacity of the Stadium, other than Kauffman Stadium, designed by Charles Deaton ?', \n             'What was the island, on which Marinelli Glacier is located, formerly known as?', \n             'The American Sweetgum is the hostplant of what kind of bug?', \n             'The name of the Japanese rock band T-Bolan was inspired by the name of an English rock band formed in what year?',\n             \n             'symptoms of pregnancy before a missed period', 'monoclonal antibodies biology definition', \n             'what is iron sulfate', \"who sang one day i'll fly away\", \n             'describe the antebellum reform movement period'\n            ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Retrieve Candidates from DB **##","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}