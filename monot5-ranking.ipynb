{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:08:17.612335Z","iopub.execute_input":"2025-01-03T18:08:17.612559Z","iopub.status.idle":"2025-01-03T18:08:24.241676Z","shell.execute_reply.started":"2025-01-03T18:08:17.612537Z","shell.execute_reply":"2025-01-03T18:08:24.240351Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\n\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:37:15.036878Z","iopub.execute_input":"2025-01-01T20:37:15.037233Z","iopub.status.idle":"2025-01-01T20:37:16.277844Z","shell.execute_reply.started":"2025-01-01T20:37:15.037204Z","shell.execute_reply":"2025-01-01T20:37:16.276945Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chunk and Index data into DB ##","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom transformers import AutoTokenizer\n\nfrom datasets import load_dataset\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nretrieval_model = \"BAAI/LLM-Embedder\"\n# retrieval_model = \"sentence-transformers/all-miniLM-L6-v2\"\n\ntokenizer = AutoTokenizer.from_pretrained(retrieval_model)\n\nTOKEN_LIMIT = 512\nSLIDING_WINDOW_OVERLAP = 100\n\n# Function for chunking with token limit and sliding window\ndef chunk_with_token_limit(text, token_limit, overlap):\n    sentences = sent_tokenize(text)  # Split text into sentences\n    chunks = []  # Store resulting chunks\n    current_chunk = []  # Temporarily hold sentences for the current chunk\n    current_chunk_tokens = 0  # Token count for the current chunk\n\n    for sentence in sentences:\n        # Tokenize the sentence and calculate its token count\n        sentence_tokens = tokenizer.tokenize(sentence)\n        num_tokens = len(sentence_tokens)\n\n        # print(f\"Tokens: {sentence_tokens[0]}\")\n\n        # If adding this sentence exceeds the token limit\n        if current_chunk_tokens + num_tokens > token_limit:\n            # Save the current chunk\n            chunk_text = \" \".join(current_chunk)\n            chunks.append(chunk_text)\n\n            # Prepare the next chunk with overlap\n            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n\n        # Add the sentence to the current chunk\n        current_chunk.append(sentence)\n        current_chunk_tokens += num_tokens\n\n    # Add the last chunk if it exists\n    if current_chunk:\n        chunk_text = \" \".join(current_chunk)\n        chunks.append(chunk_text)\n\n    return chunks\n\ndef process_document_with_identifiers(document):\n    processed_data = []\n    title_count = -1  # to start from 0\n    # print(\"document>>>>>>>\",document)\n    for section in document:\n        section_chunks = []\n        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n        title_count += 1  # Increment title count\n\n        # Tokenize the section into sentences\n        sentences = sent_tokenize(section)\n        for sentence in sentences:\n            if sentence.startswith(\"Title:\"):\n                # New document detected\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n                passage_count = [ord('a')]  # Reset passage count for the new document\n            else:\n                # Sentence under the current document\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                #print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n\n                # Increment passage_count intelligently\n                i = len(passage_count) - 1\n                while i >= 0:\n                    passage_count[i] += 1\n                    if passage_count[i] > ord('z'):\n                        passage_count[i] = ord('a')\n                        if i == 0:\n                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n                        i -= 1\n                    else:\n                        break\n\n\n        # print(\"section_chunks>>>>>>>\",section_chunks)\n        processed_data.append(section_chunks)\n\n    return processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:37:21.412495Z","iopub.execute_input":"2025-01-01T20:37:21.412936Z","iopub.status.idle":"2025-01-01T20:37:26.867016Z","shell.execute_reply.started":"2025-01-01T20:37:21.412908Z","shell.execute_reply":"2025-01-01T20:37:26.866412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Check uniqueness of data before insertion** ##","metadata":{}},{"cell_type":"code","source":"import hashlib\n\n# Function to generate a hash based on content and key metadata\ndef generate_hash(content, metadata):\n    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes(collection):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n    existing_hashes = set()\n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes_milvus(all_records):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    existing_hashes = set()\n    print(f\"all records >>> {len(all_records)}\")    \n    if all_records == None or len(all_records) == 0:\n        return existing_hashes\n        \n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadata\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:37:32.556897Z","iopub.execute_input":"2025-01-01T20:37:32.557455Z","iopub.status.idle":"2025-01-01T20:37:32.565514Z","shell.execute_reply.started":"2025-01-01T20:37:32.557424Z","shell.execute_reply":"2025-01-01T20:37:32.564312Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Store and retrieve data from Milvus** ##","metadata":{}},{"cell_type":"code","source":"!pip install pymilvus pymilvus[model]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:37:38.169217Z","iopub.execute_input":"2025-01-01T20:37:38.169547Z","iopub.status.idle":"2025-01-01T20:37:45.238318Z","shell.execute_reply.started":"2025-01-01T20:37:38.169522Z","shell.execute_reply":"2025-01-01T20:37:45.237224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom pymilvus import connections\nfrom pymilvus import FieldSchema, CollectionSchema, DataType, Collection\nfrom pymilvus import MilvusClient\nfrom pymilvus import utility\n\nclass VectorDataStore:\n    db_url = \"http://localhost:19530\"\n    #description = f\"collection created for {self.name}\"\n\n    def __init__(self, path=\"/content/ragbench.db\"):\n        self.client = MilvusClient(path)\n\n\n\n    def create_collection(self, name, vec_dim=128):\n        if self.client.has_collection(name):\n            self.default_collection_name = name\n\n        self.description = f\"collection to store {name}\"\n\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embedding\",\n            index_type=\"AUTOINDEX\",\n            # params={\n            #     \"M\": 16, # Number of bidirectional links created for each element\n            #     \"efConstruction\": 200 # Size of the dynamic list for the nearest neighbours during indexing\n            # },\n            metric_type=\"COSINE\"\n        )\n        schema = self.client.create_schema(\n            auto_id=False,\n            enable_dynamic_fields=True,\n        )\n        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n        schema.add_field(field_name=\"timestamp\", datatype=DataType.INT64)\n        \n        collection = self.client.create_collection(collection_name=name,\n                                       schema=schema,\n                                       index_params=index_params)\n        self.current_collection = collection\n        return collection\n\n\n    def get_collection(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exist.\")\n        self.current_collection = Collection(name)\n        return self.current_collection\n\n    def get_all_records(self, collection):\n        all_records = self.client.query(\n            collection_name=collection,\n            filter=None,\n            output_fields=[\"documents\", \"metadata\"],\n            limit=10000\n        )\n        if all_records == None:\n            all_records = []\n\n        return all_records\n\n    def has_entities(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exists.\")\n        self.default_collection = name\n        collection_stats = self.client.get_collection_stats(collection_name)\n        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n        return count\n\n    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n                documents: list[str], embeddings: np.ndarray, ids: list[int]):\n\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n\n        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n\n        data = []\n        for meta, doc, emb, id in zip(metadata, documents, embeddings, ids):\n          datum = {\n              \"pk\": id,\n              \"metadata\": meta,\n              \"documents\": doc,\n              \"embedding\": emb.tolist(),\n              \"timestamp\": int(time.time()),\n          }\n          data.append(datum)\n\n        self.client.insert(collection_name, data)\n        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n\n    def drop_collection(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.drop_collection(collection_name)\n        print(f\"Dropped collection '{collection_name}'.\")\n\n    def delete_all(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.delete(collection_name, expr=\"pk >= 0\")\n        self.client.flush([collection_name])\n\n    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> list[dict[str, any]]:\n        \"\"\"\n        Search across all collections for the top-k closest embeddings.\n        :param query_embedding: The embedding vector to search for.\n        :param top_k: Number of top results to retrieve.\n        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n        \"\"\"\n        results = []\n        #collections = self.client.list_collections()\n        collections = [\"ragbench_collection_techqa_v09\"]\n        start_time = time.time()\n        for collection_name in collections:\n            if not self.client.has_collection(collection_name):\n                continue\n\n            # Set params to COSINE to match chromadb\n            search_params = {\n                \"metric_type\": \"COSINE\", \n                \"params\": {\n                    \"ef\": 64\n                }\n            }\n\n            search_results = self.client.search(\n                collection_name=collection_name,\n                data=[query_embedding],\n                anns_field=\"embedding\",\n                search_params=search_params,\n                limit=top_k,\n                output_fields=[\"metadata\", \"documents\"]\n            )\n\n            for hits in search_results:\n                for hit in hits:\n                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n                    results.append({\n                        \"collection\": collection_name,\n                        \"id\": hit[\"id\"],\n                        \"metadata\": hit[\"entity\"][\"metadata\"],\n                        \"distance\": hit[\"distance\"],\n                        \"documents\": hit[\"entity\"][\"documents\"]\n                      })\n\n        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n        end_time = time.time()\n        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n        return results\n\n    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n      \"\"\"\n      Extract embedding values from search results.\n      :param search_results: List of dictionaries containing search results.\n      :return: List of embedding vectors as NumPy arrays.\n      \"\"\"\n      return [np.array(result[\"documents\"]) for result in search_results if \"documents\" in result]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:44:21.683690Z","iopub.execute_input":"2025-01-01T20:44:21.684017Z","iopub.status.idle":"2025-01-01T20:44:21.700505Z","shell.execute_reply.started":"2025-01-01T20:44:21.683995Z","shell.execute_reply":"2025-01-01T20:44:21.699650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\ndatasets = ['hagrid', 'hotpotqa', 'msmarco']\n\nall_documents = []\nall_ids = []\nall_metadatas = []\n\n# Process each dataset\ndoc_idx = 0  # Global document index for unique IDs\nfor dataset in datasets:\n    data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n    # #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n    # data = data.select(range(2))\n    for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n        # Extract document text\n        doc_text = row.get('documents', '')\n\n        # Skip if no documents found\n        if not doc_text:\n            continue\n\n        # Process the document\n        processed_output = process_document_with_identifiers(doc_text)\n        added_item_idxs = set()\n\n        # Populate the lists\n        for section_idx, section in enumerate(processed_output):\n            for item_idx, (prefix, content) in enumerate(section):\n                # Skip if this item_idx has already been processed\n                if item_idx in added_item_idxs:\n                    continue\n\n                # Add the item_idx to the set to track it\n                added_item_idxs.add(item_idx)\n\n                # Add the document\n                document = f\"[{prefix}] {content}\"\n                all_documents.append(document)\n\n                # Construct a globally unique ID\n                doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n                all_ids.append(doc_id)\n\n                # Construct metadata\n                metadata = {\n                    \"dataset\": dataset,\n                    \"global_index\": doc_idx,\n                    \"section_index\": section_idx,\n                    \"item_index\": item_idx,\n                    \"prefix\": prefix,\n                    \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n                }\n                all_metadatas.append(metadata)\n\n        doc_idx += 1  # Increment global document index\n\n# Step 4: Generate Embeddings\n#embedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nembedder = SentenceTransformer(retrieval_model)  # Pretrained sentence transformer\nbatch_size = 2500  # Adjust based on available memory\n\n# Generate embeddings in batches\nall_embeddings = []\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n    batch_docs = all_documents[i:i + batch_size]\n    batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n    all_embeddings.extend(batch_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:38:00.040672Z","iopub.execute_input":"2025-01-01T20:38:00.041022Z","iopub.status.idle":"2025-01-01T20:40:33.388196Z","shell.execute_reply.started":"2025-01-01T20:38:00.040994Z","shell.execute_reply":"2025-01-01T20:40:33.387286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"collection_name = \"ragbench_hhm_v1\"\ndatastor = VectorDataStore()\n\ninsert_data = False\nstore_client = \"Milvus\"\nnum_records = 0\n\nif datastor.client.has_collection(collection_name):\n  num_records = datastor.has_entities(collection_name)\n  if num_records == 0:\n    insert_data = True\nelse:\n  datastor.create_collection(collection_name, embedder.get_sentence_embedding_dimension())\n  insert_data = True\n\nprint(f\"count >>> {num_records} insert_data >>> {insert_data}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:44:34.632625Z","iopub.execute_input":"2025-01-01T20:44:34.632917Z","iopub.status.idle":"2025-01-01T20:44:34.644830Z","shell.execute_reply.started":"2025-01-01T20:44:34.632893Z","shell.execute_reply":"2025-01-01T20:44:34.644071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding data to Milvus with enhanced duplicate check\nif not insert_data:\n    existing_hashes = set()\nelse:\n    existing_hashes = get_existing_hashes_milvus(datastor.get_all_records(collection_name))\n\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to Milvus\"):\n    batch_embeddings = all_embeddings[i:i + batch_size]\n    batch_metadatas = all_metadatas[i:i + batch_size]\n    batch_documents = all_documents[i:i + batch_size]\n    batch_ids = []\n\n    # Generate hashes for each document in the batch\n    for doc, metadata in zip(batch_documents, batch_metadatas):\n        doc_hash = generate_hash(doc, metadata)\n        if doc_hash not in existing_hashes:\n            batch_ids.append(doc_hash)\n            existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n        else:\n            print(f\"Skipping duplicate document: {doc[:50]}...\")  # Print a preview of the duplicate doc\n\n    # Add non-duplicate documents to the database\n    if batch_ids:  # Ensure there are non-duplicate documents to add\n        # Add the batch to the Milvus collection\n        if store_client == \"Milvus\" and insert_data:\n            datastor.insert(collection_name,\n                metadata=batch_metadatas,\n                documents=batch_documents,\n                embeddings=np.array(batch_embeddings),\n                ids=batch_ids\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:44:40.064437Z","iopub.execute_input":"2025-01-01T20:44:40.064753Z","iopub.status.idle":"2025-01-01T20:46:49.963801Z","shell.execute_reply.started":"2025-01-01T20:44:40.064725Z","shell.execute_reply":"2025-01-01T20:46:49.962864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions = ['When was Rolex founded?', 'How large is the region of Macedonia?', \n             'Where is GMT Games headquartered?', 'What state is directly north of North Carolina?', \n             'When was Brown v. Board of Education?',\n             \n             'What star of Parks and Recreation appeared in November?', \n             'What is the capacity of the Stadium, other than Kauffman Stadium, designed by Charles Deaton ?', \n             'What was the island, on which Marinelli Glacier is located, formerly known as?', \n             'The American Sweetgum is the hostplant of what kind of bug?', \n             'The name of the Japanese rock band T-Bolan was inspired by the name of an English rock band formed in what year?',\n             \n             'symptoms of pregnancy before a missed period', 'monoclonal antibodies biology definition', \n             'what is iron sulfate', \"who sang one day i'll fly away\", \n             'describe the antebellum reform movement period'\n            ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:09:25.097782Z","iopub.execute_input":"2025-01-03T18:09:25.098170Z","iopub.status.idle":"2025-01-03T18:09:25.103413Z","shell.execute_reply.started":"2025-01-03T18:09:25.098115Z","shell.execute_reply":"2025-01-03T18:09:25.102265Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## **Retrieve Candidates from DB** ##","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Load monoT5 model** ###","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load MonoT5 model and tokenizer\nmodel_name = \"castorini/monot5-base-msmarco\"  # or a suitable MonoT5 variant\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:15:19.932602Z","iopub.execute_input":"2025-01-03T18:15:19.932963Z","iopub.status.idle":"2025-01-03T18:16:08.035694Z","shell.execute_reply.started":"2025-01-03T18:15:19.932935Z","shell.execute_reply":"2025-01-03T18:16:08.034189Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb52ae3426374820b6ebbb7cafaaa6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602a3d9bf6a745979e32c254e0efa950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecdbcf8c5c1842b196636286db08f614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ff8423caa245b6b3fb38a4c358acea"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20234e9c34f414db39741abd98edc44"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### **Prepare Query-Document Pairs** ###","metadata":{}},{"cell_type":"code","source":"query = \"What is the capital of France?\"\ndocuments = [\n    \"Paris is the capital city of France.\",\n    \"The Eiffel Tower is in France.\",\n    \"Berlin is a city in Germany.\"\n]\n\n# Prepare inputs\ninputs = [f\"Query: {query} Document: {doc}\" for doc in documents]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:16:41.517057Z","iopub.execute_input":"2025-01-03T18:16:41.517570Z","iopub.status.idle":"2025-01-03T18:16:41.524314Z","shell.execute_reply.started":"2025-01-03T18:16:41.517529Z","shell.execute_reply":"2025-01-03T18:16:41.522625Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### **Tokenize the Inputs** ###","metadata":{}},{"cell_type":"code","source":"# Tokenize inputs\ntokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:16:45.104262Z","iopub.execute_input":"2025-01-03T18:16:45.104641Z","iopub.status.idle":"2025-01-03T18:16:45.113403Z","shell.execute_reply.started":"2025-01-03T18:16:45.104615Z","shell.execute_reply":"2025-01-03T18:16:45.112093Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### **Get Relevance Scores** ###","metadata":{}},{"cell_type":"code","source":"# Get model predictions\noutputs = model.generate(**tokenized_inputs)\n\n# Decode outputs to get relevance scores (e.g., 'true' or 'false')\npredictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n\n# Convert predictions to scores\nscores = [1.0 if pred.lower() == \"true\" else 0.0 for pred in predictions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:16:51.748295Z","iopub.execute_input":"2025-01-03T18:16:51.748730Z","iopub.status.idle":"2025-01-03T18:16:52.239868Z","shell.execute_reply.started":"2025-01-03T18:16:51.748695Z","shell.execute_reply":"2025-01-03T18:16:52.238568Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### **Rank the Documents** ###","metadata":{}},{"cell_type":"code","source":"# Rank documents\nranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n\n# Output ranked documents\nfor i, (doc, score) in enumerate(ranked_docs, start=1):\n    print(f\"Rank {i}: {doc} (Score: {score})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T18:17:13.039551Z","iopub.execute_input":"2025-01-03T18:17:13.039984Z","iopub.status.idle":"2025-01-03T18:17:13.047202Z","shell.execute_reply.started":"2025-01-03T18:17:13.039947Z","shell.execute_reply":"2025-01-03T18:17:13.046000Z"}},"outputs":[{"name":"stdout","text":"Rank 1: Paris is the capital city of France. (Score: 1.0)\nRank 2: The Eiffel Tower is in France. (Score: 0.0)\nRank 3: Berlin is a city in Germany. (Score: 0.0)\n","output_type":"stream"}],"execution_count":9}]}