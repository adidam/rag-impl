{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install milvus milvus[model] pymilvus pymilvus[model]\n!pip install sentence_transformers datasets accelerate\n!pip install torch transformers\n\n# !rm -rf /kaggle/working/ragbench.db","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:54:52.511974Z","iopub.execute_input":"2025-01-13T02:54:52.512317Z","iopub.status.idle":"2025-01-13T02:55:05.407811Z","shell.execute_reply.started":"2025-01-13T02:54:52.512287Z","shell.execute_reply":"2025-01-13T02:55:05.406769Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: milvus in /usr/local/lib/python3.10/dist-packages (2.3.5)\nRequirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.5.3)\n\u001b[33mWARNING: milvus 2.3.5 does not provide the extra 'model'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (71.0.4)\nRequirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.64.1)\nRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.0.1)\nRequirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\nRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.1.4)\nRequirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.11)\nRequirement already satisfied: milvus-model>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus[model]) (0.2.11)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.66.5)\nRequirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.44.2)\nRequirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.20.1)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.24.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.19.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (24.3.25)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (4.12.2)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (10.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\nRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:55:31.041033Z","iopub.execute_input":"2025-01-13T02:55:31.041351Z","iopub.status.idle":"2025-01-13T02:55:31.059798Z","shell.execute_reply.started":"2025-01-13T02:55:31.041327Z","shell.execute_reply":"2025-01-13T02:55:31.058790Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom transformers import AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BAAI/LLM-Embedder\")\n#tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Initialize the model\nembedder = SentenceTransformer(\"BAAI/LLM-Embedder\") \n# embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:55:34.546794Z","iopub.execute_input":"2025-01-13T02:55:34.547134Z","iopub.status.idle":"2025-01-13T02:55:36.084509Z","shell.execute_reply.started":"2025-01-13T02:55:34.547105Z","shell.execute_reply":"2025-01-13T02:55:36.083834Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# New code - 12/4 10 pm\n\nfrom nltk.tokenize import sent_tokenize\nfrom transformers import AutoTokenizer\n\n# Sliding window configuration\nTOKEN_LIMIT = 512\nSLIDING_WINDOW_OVERLAP = 100  # Overlap between consecutive chunks (in tokens)\n\n# Function for chunking with token limit and sliding window\ndef chunk_with_token_limit(text, token_limit, overlap):\n    sentences = sent_tokenize(text)  # Split text into sentences\n    chunks = []  # Store resulting chunks\n    current_chunk = []  # Temporarily hold sentences for the current chunk\n    current_chunk_tokens = 0  # Token count for the current chunk\n\n    for sentence in sentences:\n        # Tokenize the sentence and calculate its token count\n        sentence_tokens = tokenizer.tokenize(sentence)\n        num_tokens = len(sentence_tokens)\n\n        # print(f\"Tokens: {sentence_tokens[0]}\")\n\n        # If adding this sentence exceeds the token limit\n        if current_chunk_tokens + num_tokens > token_limit:\n            # Save the current chunk\n            chunk_text = \" \".join(current_chunk)\n            chunks.append(chunk_text)\n\n            # Prepare the next chunk with overlap\n            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n\n        # Add the sentence to the current chunk\n        current_chunk.append(sentence)\n        current_chunk_tokens += num_tokens\n\n    # Add the last chunk if it exists\n    if current_chunk:\n        chunk_text = \" \".join(current_chunk)\n        chunks.append(chunk_text)\n\n    return chunks\n\ndef process_document_with_identifiers(document):\n    processed_data = []\n    title_count = -1  # to start from 0\n    # print(\"document>>>>>>>\",document)\n    for section in document:\n        section_chunks = []\n        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n        title_count += 1  # Increment title count\n\n        # Tokenize the section into sentences\n        sentences = sent_tokenize(section)\n        for sentence in sentences:\n            if sentence.startswith(\"Title:\"):\n                # New document detected\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n                passage_count = [ord('a')]  # Reset passage count for the new document\n            else:\n                # Sentence under the current document\n                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n                #print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n                for chunk in chunked_texts:\n                    section_chunks.append([identifier, chunk])\n\n                # Increment passage_count intelligently\n                i = len(passage_count) - 1\n                while i >= 0:\n                    passage_count[i] += 1\n                    if passage_count[i] > ord('z'):\n                        passage_count[i] = ord('a')\n                        if i == 0:\n                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n                        i -= 1\n                    else:\n                        break\n\n\n        # print(\"section_chunks>>>>>>>\",section_chunks)\n        processed_data.append(section_chunks)\n\n    return processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:55:53.256037Z","iopub.execute_input":"2025-01-13T02:55:53.256369Z","iopub.status.idle":"2025-01-13T02:55:53.266282Z","shell.execute_reply.started":"2025-01-13T02:55:53.256341Z","shell.execute_reply":"2025-01-13T02:55:53.265342Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### **Class to handle data in Milvus** ###","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom pymilvus import connections\nfrom pymilvus import FieldSchema, CollectionSchema, DataType, Collection\nfrom pymilvus import MilvusClient\nfrom pymilvus import utility\n\nclass VectorDataStore:\n    db_url = \"http://localhost:19530\"\n\n    #description = f\"collection created for {self.name}\"\n\n    def __init__(self, path=\"/content/ragbench.db\"):\n        self.client = MilvusClient(path)\n\n\n    def get_or_create_collection(self, name, vec_dim=128):\n        try:\n            self.get_collection(name)\n        except:\n            print(f\"Collection {name} doesn't exist. Creating...\")\n            self.create_collection(name, vec_dim)\n        \n\n    def create_collection(self, name, vec_dim=128):\n        if self.client.has_collection(name):\n            self.default_collection_name = name\n\n        self.description = f\"collection to store {name}\"\n\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embedding\",\n            index_type=\"AUTOINDEX\",\n            metric_type=\"COSINE\"\n        )\n        schema = self.client.create_schema(\n            auto_id=False,\n            enable_dynamic_fields=True,\n        )\n        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n\n        collection = self.client.create_collection(collection_name=name,\n                                       schema=schema,\n                                       index_params=index_params)\n        self.current_collection = collection\n        return collection\n\n\n    def get_collection(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exist.\")\n        self.current_collection = Collection(name)\n        return self.current_collection\n\n    def get_all_records(self, collection):\n        all_records = self.client.query(\n            collection_name=collection,\n            filter=None,\n            output_fields=[\"documents\", \"metadata\"],\n            limit=10000\n        )\n        if all_records == None:\n            all_records = []\n\n        return all_records\n\n    def has_entities(self, name):\n        if not self.client.has_collection(name):\n            raise ValueError(f\"Collection '{name}' does not exists.\")\n        self.default_collection = name\n        collection_stats = self.client.get_collection_stats(collection_name)\n        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n        return count\n\n    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n                documents: list[str], embeddings: np.ndarray, ids: list[int]):\n\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n\n        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n\n        data = []\n        for meta, doc, emb, id in zip(metadata, documents, embeddings, ids):\n          datum = {\n              \"pk\": id,\n              \"metadata\": meta,\n              \"documents\": doc,\n              \"embedding\": emb.tolist(),\n          }\n          data.append(datum)\n\n        self.client.insert(collection_name, data)\n        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n\n    def drop_collection(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.drop_collection(collection_name)\n        print(f\"Dropped collection '{collection_name}'.\")\n\n    def delete_all(self, collection_name: str):\n        if not self.client.has_collection(collection_name):\n            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n        self.client.delete(collection_name, expr=\"pk >= 0\")\n        self.client.flush([collection_name])\n\n    def search(self, query_embedding: np.ndarray, top_k: int = 10) -> list[dict[str, any]]:\n        \"\"\"\n        Search across all collections for the top-k closest embeddings.\n        :param query_embedding: The embedding vector to search for.\n        :param top_k: Number of top results to retrieve.\n        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n        \"\"\"\n        results = []\n        #collections = self.client.list_collections()\n        collections = [\"ragbench_collection_techqa_v09\"]\n        start_time = time.time()\n        for collection_name in collections:\n            if not self.client.has_collection(collection_name):\n                continue\n\n            # Set params to COSINE to match chromadb\n            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n\n            search_results = self.client.search(\n                collection_name=collection_name,\n                data=[query_embedding],\n                anns_field=\"embedding\",\n                search_params=search_params,\n                limit=top_k,\n                output_fields=[\"metadata\", \"documents\"]\n            )\n\n            for hits in search_results:\n                for hit in hits:\n                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n                    results.append({\n                        \"collection\": collection_name,\n                        \"id\": hit[\"id\"],\n                        \"metadata\": hit[\"entity\"][\"metadata\"],\n                        \"distance\": hit[\"distance\"],\n                        \"documents\": hit[\"entity\"][\"documents\"]\n                      })\n\n        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n        end_time = time.time()\n        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n        return results\n\n    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n      \"\"\"\n      Extract embedding values from search results.\n      :param search_results: List of dictionaries containing search results.\n      :return: List of embedding vectors as NumPy arrays.\n      \"\"\"\n      return [np.array(result[\"documents\"]) for result in search_results if \"documents\" in result]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:56:05.157118Z","iopub.execute_input":"2025-01-13T02:56:05.157430Z","iopub.status.idle":"2025-01-13T02:56:05.173452Z","shell.execute_reply.started":"2025-01-13T02:56:05.157405Z","shell.execute_reply":"2025-01-13T02:56:05.172529Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"#### **Functions to generate and retrieve hashes** ####","metadata":{}},{"cell_type":"code","source":"import hashlib\n\n# Function to generate a hash based on content and key metadata\ndef generate_hash(content, metadata):\n    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n\n# Function to retrieve existing hashes from the database\ndef get_existing_hashes_milvus(all_records):\n    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n    existing_hashes = set()\n    print(f\"all records >>> {len(all_records)}\")    \n    if all_records == None or len(all_records) == 0:\n        return existing_hashes\n        \n    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadata\"]):\n        doc_hash = generate_hash(doc, metadata)\n        existing_hashes.add(doc_hash)\n    return existing_hashes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:56:23.706739Z","iopub.execute_input":"2025-01-13T02:56:23.707076Z","iopub.status.idle":"2025-01-13T02:56:23.712272Z","shell.execute_reply.started":"2025-01-13T02:56:23.707046Z","shell.execute_reply":"2025-01-13T02:56:23.711416Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### Tokenize and generate documents ###","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\ndatasets = [\n    'covidqa', 'cuad', 'delucionqa', 'emanual', 'expertqa',\n    'finqa', 'hagrid', 'hotpotqa', 'msmarco', 'pubmedqa',\n    'tatqa', 'techqa'\n]\n\n# Initialize storage for documents, IDs, and metadata\nall_documents = []\nall_ids = []\nall_metadatas = []\n\n# Process each dataset\ndoc_idx = 0  # Global document index for unique IDs\nfor dataset in datasets:\n    data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n    # only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n    # data = data.select(range(5))\n    for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n        # Extract document text\n        doc_text = row.get('documents', '')\n\n        # Skip if no documents found\n        if not doc_text:\n            continue\n\n        # Process the document\n        processed_output = process_document_with_identifiers(doc_text)\n        added_item_idxs = set()\n\n        # Populate the lists\n        for section_idx, section in enumerate(processed_output):\n            for item_idx, (prefix, content) in enumerate(section):\n                # Skip if this item_idx has already been processed\n                if item_idx in added_item_idxs:\n                    continue\n\n                # Add the item_idx to the set to track it\n                added_item_idxs.add(item_idx)\n\n                # Add the document\n                document = f\"[{prefix}] {content}\"\n                all_documents.append(document)\n\n                # Construct a globally unique ID\n                doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n                all_ids.append(doc_id)\n\n                # Construct metadata\n                metadata = {\n                    \"dataset\": dataset,\n                    \"global_index\": doc_idx,\n                    \"section_index\": section_idx,\n                    \"item_index\": item_idx,\n                    \"prefix\": prefix,\n                    \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n                }\n                all_metadatas.append(metadata)\n\n        doc_idx += 1  # Increment global document index\n\n# Step 4: Generate Embeddings\nbatch_size = 2500  # Adjust based on available memory\n\n# Generate embeddings in batches\nall_embeddings = []\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n    batch_docs = all_documents[i:i + batch_size]\n    batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n    all_embeddings.extend(batch_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T02:56:45.385029Z","iopub.execute_input":"2025-01-13T02:56:45.385382Z","execution_failed":"2025-01-13T03:49:57.918Z"}},"outputs":[{"name":"stderr","text":"Processing covidqa: 1252it [00:05, 237.98it/s]\nProcessing cuad: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\nProcessing cuad: 1530it [01:55, 13.29it/s]\nProcessing delucionqa: 1460it [00:12, 112.52it/s]\nProcessing emanual: 1054it [00:06, 168.08it/s]\nProcessing expertqa: 1621it [00:29, 54.31it/s]\nProcessing finqa: 12502it [01:26, 144.48it/s]\nProcessing hagrid: 2892it [00:11, 252.07it/s]\nProcessing hotpotqa: 1883it [00:07, 241.27it/s]\nProcessing msmarco: 1870it [00:14, 128.36it/s]\nProcessing pubmedqa: 16431it [00:59, 261.76it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### initialize milvus and add data to db ###","metadata":{}},{"cell_type":"code","source":"datastor = VectorDataStore('/kaggle/working/ragbench.db')\ncollection_name = \"ragbench_collection_all_baai_v1\"\ninsert_data = False\nstore_client = \"Milvus\"\nnum_records = 0\n\nvector_dim = embedder.get_sentence_embedding_dimension()\n\ndatastor.get_or_create_collection(collection_name, vector_dim)\nnum_records = datastor.has_entities(collection_name)\nif num_records == 0:\n    insert_data = True\n\nprint(f\"count >>> {num_records} insert_data >>> {insert_data}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-13T03:49:57.919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"existing_hashes = get_existing_hashes_milvus(datastor.get_all_records(collection_name))\n\nfor i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to DB\"):\n    batch_embeddings = all_embeddings[i:i + batch_size]\n    batch_metadatas = all_metadatas[i:i + batch_size]\n    batch_documents = all_documents[i:i + batch_size]\n    batch_ids = []\n\n    # Generate hashes for each document in the batch\n    for doc, metadata in zip(batch_documents, batch_metadatas):\n        doc_hash = generate_hash(doc, metadata)\n        if doc_hash not in existing_hashes:\n            batch_ids.append(doc_hash)\n            existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n        else:\n            print(f\"Skipping duplicate document: {doc[:15]}...\")  # Print a preview of the duplicate doc\n\n    # Add non-duplicate documents to the database\n    if batch_ids:  # Ensure there are non-duplicate documents to add\n        # Add the batch to the Milvus collection\n        if store_client == \"Milvus\" and insert_data:\n            datastor.insert(collection_name,\n                metadata=batch_metadatas,\n                documents=batch_documents,\n                embeddings=np.array(batch_embeddings),\n                ids=batch_ids\n            )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-13T03:49:57.919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"total records in datastore {datastor.has_entities(collection_name)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-13T03:49:57.919Z"}},"outputs":[],"execution_count":null}]}