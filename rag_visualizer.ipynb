{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidam/rag-impl/blob/main/rag_visualizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjIAeZQ6PZD",
        "outputId": "23ce8f74-5ec4-40fe-f42d-efc074ad3e91",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.16.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.5.4)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (4.25.5)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: milvus-model>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus[model]) (0.2.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.47.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.20.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.4.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (4.12.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.18.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.35)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (0.2.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!pip install datasets\n",
        "!pip install gradio\n",
        "!pip install pymilvus pymilvus[model]\n",
        "!pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NIN5yZ2mhjoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c824c1-1eed-4483-d83f-6e360acae78e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and other utility functions**"
      ],
      "metadata": {
        "id": "7YUOuy1bjA4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_model = \"BAAI/LLM-Embedder\"\n",
        "# embedding_model = \"BAAI/bge-large-en\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer"
      ],
      "metadata": {
        "id": "qB-77iRMj4t9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cffd1a61-bd3d-4f73-8b88-4e5fbde3b31f",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New code - 12/4 10 pm\n",
        "import hashlib\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sliding window configuration\n",
        "TOKEN_LIMIT = 512\n",
        "SLIDING_WINDOW_OVERLAP = 100  # Overlap between consecutive chunks (in tokens)\n",
        "\n",
        "# Function for chunking with token limit and sliding window\n",
        "def chunk_with_token_limit(text, token_limit=512, overlap=100):\n",
        "    sentences = sent_tokenize(text)  # Split text into sentences\n",
        "    chunks = []  # Store resulting chunks\n",
        "    current_chunk = []  # Temporarily hold sentences for the current chunk\n",
        "    current_chunk_tokens = 0  # Token count for the current chunk\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence and calculate its token count\n",
        "        sentence_tokens = tokenizer.tokenize(sentence)\n",
        "        num_tokens = len(sentence_tokens)\n",
        "\n",
        "        # print(f\"Tokens: {sentence_tokens[0]}\")\n",
        "\n",
        "        # If adding this sentence exceeds the token limit\n",
        "        if current_chunk_tokens + num_tokens > token_limit:\n",
        "            # Save the current chunk\n",
        "            chunk_text = \" \".join(current_chunk)\n",
        "            chunks.append(chunk_text)\n",
        "\n",
        "            # Prepare the next chunk with overlap\n",
        "            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n",
        "            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n",
        "            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_chunk_tokens += num_tokens\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk:\n",
        "        chunk_text = \" \".join(current_chunk)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_document_with_identifiers(document):\n",
        "    processed_data = []\n",
        "    title_count = -1  # to start from 0\n",
        "    # print(\"document>>>>>>>\",document)\n",
        "    for section in document:\n",
        "        section_chunks = []\n",
        "        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n",
        "        title_count += 1  # Increment title count\n",
        "\n",
        "        # Tokenize the section into sentences\n",
        "        sentences = sent_tokenize(section)\n",
        "        for sentence in sentences:\n",
        "            if sentence.startswith(\"Title:\"):\n",
        "                # New document detected\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "                passage_count = [ord('a')]  # Reset passage count for the new document\n",
        "            else:\n",
        "                # Sentence under the current document\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "                #print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "\n",
        "                # Increment passage_count intelligently\n",
        "                i = len(passage_count) - 1\n",
        "                while i >= 0:\n",
        "                    passage_count[i] += 1\n",
        "                    if passage_count[i] > ord('z'):\n",
        "                        passage_count[i] = ord('a')\n",
        "                        if i == 0:\n",
        "                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n",
        "                        i -= 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "\n",
        "        #print(\"section_chunks>>>>>>>\",section_chunks)\n",
        "        processed_data.append(section_chunks)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Function to generate a hash based on content and key metadata\n",
        "def generate_hash(content, metadata):\n",
        "    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n",
        "    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n",
        "    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes(collection):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n",
        "    existing_hashes = set()\n",
        "    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes_milvus(all_records):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    existing_hashes = set()\n",
        "    print(f\"all records >>> {len(all_records)}\")\n",
        "    if all_records == None or len(all_records) == 0:\n",
        "        return existing_hashes\n",
        "\n",
        "    for record in all_records:\n",
        "        doc = record.get(\"documents\")\n",
        "        metadata = record.get(\"metadata\")\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "\n",
        "# Tokenize corpus and prepare BM25 encoder\n",
        "def prepare_bm25_encoder(texts):\n",
        "    tokenized_corpus = [word_tokenize(text.lower()) for text in texts]\n",
        "    bm25_encoder = BM25Okapi(tokenized_corpus)\n",
        "    return bm25_encoder\n",
        "\n",
        "def generate_sparse_vector_bm25(query, bm25_encoder):\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    scores = bm25_encoder.get_scores(tokenized_query)\n",
        "    # Convert scores to CSR format\n",
        "    sparse_vector = sp.csr_matrix(scores)\n",
        "    return sparse_vector"
      ],
      "metadata": {
        "id": "pK2czU0-hoHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milvus VectorDataStore class**"
      ],
      "metadata": {
        "id": "J5QuDMffi2K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from pymilvus import connections\n",
        "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
        "from pymilvus import MilvusClient\n",
        "from pymilvus import utility\n",
        "\n",
        "class VectorDataStore:\n",
        "    db_url = \"http://localhost:19530\"\n",
        "\n",
        "    #description = f\"collection created for {self.name}\"\n",
        "\n",
        "    def __init__(self, path=\"/content/ragbench.db\"):\n",
        "        self.client = MilvusClient(path)\n",
        "\n",
        "\n",
        "    def get_or_create_collection(self, name, vec_dim=128):\n",
        "        try:\n",
        "            self.get_collection(name)\n",
        "        except:\n",
        "            print(f\"Collection {name} doesn't exist. Creating...\")\n",
        "            self.create_collection(name, vec_dim)\n",
        "\n",
        "\n",
        "    def create_collection(self, name, vec_dim=128):\n",
        "        if self.client.has_collection(name):\n",
        "            self.default_collection_name = name\n",
        "\n",
        "        self.description = f\"collection to store {name}\"\n",
        "\n",
        "        index_params = self.client.prepare_index_params()\n",
        "        index_params.add_index(\n",
        "            field_name=\"embedding\",\n",
        "            index_type=\"AUTOINDEX\",\n",
        "            metric_type=\"COSINE\"\n",
        "        )\n",
        "        index_params.add_index(\n",
        "            field_name=\"sparse\",\n",
        "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
        "            metric_type=\"IP\"\n",
        "        )\n",
        "        schema = self.client.create_schema(\n",
        "            auto_id=False,\n",
        "            enable_dynamic_fields=True,\n",
        "        )\n",
        "        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n",
        "        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
        "        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n",
        "        schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
        "        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n",
        "\n",
        "        collection = self.client.create_collection(collection_name=name,\n",
        "                                       schema=schema,\n",
        "                                       index_params=index_params)\n",
        "        self.current_collection = collection\n",
        "        return collection\n",
        "\n",
        "\n",
        "    def get_collection(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exist.\")\n",
        "        self.current_collection = Collection(name)\n",
        "        return self.current_collection\n",
        "\n",
        "    def get_all_records(self, collection):\n",
        "        all_records = self.client.query(\n",
        "            collection_name=collection,\n",
        "            filter=None,\n",
        "            output_fields=[\"documents\", \"metadata\"],\n",
        "            limit=10000\n",
        "        )\n",
        "        if all_records == None:\n",
        "            all_records = []\n",
        "\n",
        "        return all_records\n",
        "\n",
        "    def has_entities(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exists.\")\n",
        "        self.default_collection = name\n",
        "        collection_stats = self.client.get_collection_stats(name)\n",
        "        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n",
        "        return count\n",
        "\n",
        "    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n",
        "                documents: list[str], sparse_embs: np.ndarray, embeddings: np.ndarray, ids: list[int]):\n",
        "\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n",
        "\n",
        "        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n",
        "           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n",
        "\n",
        "        data = []\n",
        "        for meta, doc, sp_embs, emb, id in zip(metadata, documents, sparse_embs, embeddings, ids):\n",
        "          datum = {\n",
        "              \"pk\": id,\n",
        "              \"metadata\": meta,\n",
        "              \"documents\": doc,\n",
        "              \"sparse\": sp_embs,\n",
        "              \"embedding\": emb.tolist(),\n",
        "          }\n",
        "          data.append(datum)\n",
        "\n",
        "        self.client.insert(collection_name, data)\n",
        "        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n",
        "\n",
        "    def drop_collection(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.drop_collection(collection_name)\n",
        "        print(f\"Dropped collection '{collection_name}'.\")\n",
        "\n",
        "    def delete_all(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.delete(collection_name, expr=\"pk >= 0\")\n",
        "        self.client.flush([collection_name])\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, collection='all', top_k: int = 10) -> list[dict[str, any]]:\n",
        "        \"\"\"\n",
        "        Search across all collections for the top-k closest embeddings.\n",
        "        :param query_embedding: The embedding vector to search for.\n",
        "        :param top_k: Number of top results to retrieve.\n",
        "        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        if collection=='all':\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"embedding\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def sparse_search(self, query_embedding: np.ndarray, collection='all', top_k : int=10)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        if (collection=='all'):\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"IP\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"sparse\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def hybrid_search(self, sparse_query_embedding: np.ndarray, dense_query_embedding: np.ndarray, collection='all', top_k : int=10, alpha=0.3)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "        sparse_results = self.sparse_search(sparse_query_embedding, collection, top_k)\n",
        "        n = int(len(sparse_results) * alpha)\n",
        "        alpha_sparse_results = sparse_results[:n]\n",
        "        dense_results = self.search(dense_query_embedding, collection, top_k)\n",
        "        #'results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        results = dense_results + alpha_sparse_results\n",
        "        end_time = time.time()\n",
        "        print(f\"Hybrid Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n",
        "      \"\"\"\n",
        "      Extract embedding values from search results.\n",
        "      :param search_results: List of dictionaries containing search results.\n",
        "      :return: List of embedding vectors as NumPy arrays.\n",
        "      \"\"\"\n",
        "      return [result[\"documents\"] for result in search_results if \"documents\" in result]"
      ],
      "metadata": {
        "id": "5wJcp7qOieKd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Index Data into DB**"
      ],
      "metadata": {
        "id": "ziIwM_e5yecj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['covidqa','cuad','delucionqa','emanual','expertqa','finqa','hagrid','hotpotqa','msmarco','pubmedqa','tatqa','techqa']\n",
        "datastor = VectorDataStore()\n",
        "\n",
        "insert_data = True\n",
        "store_client = \"Milvus\"\n",
        "num_records = 0\n",
        "\n",
        "vector_dim = embedder.get_sentence_embedding_dimension()"
      ],
      "metadata": {
        "id": "9Ug972iXDRwO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#** Need not run this every time"
      ],
      "metadata": {
        "id": "6OuE4g4oIFWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Initialize storage for documents, IDs, and metadata\n",
        "\n",
        "for i in range(12):\n",
        "  all_documents = []\n",
        "  all_ids = []\n",
        "  all_metadatas = []\n",
        "\n",
        "  # Process each dataset\n",
        "  doc_idx = 0  # Global document index for unique IDs\n",
        "  for dataset in [datasets[i]]:\n",
        "      milvus_collection_name = f\"ragbench_collection_{dataset}\"\n",
        "      try:\n",
        "        num_records = datastor.has_entities(milvus_collection_name)\n",
        "      except:\n",
        "        num_records = 0\n",
        "        print(\"we are good. collection doesnt exists\")\n",
        "\n",
        "      if num_records > 0:\n",
        "          datastor.drop_collection(milvus_collection_name)\n",
        "\n",
        "      datastor.get_or_create_collection(milvus_collection_name, vector_dim)\n",
        "\n",
        "      print(f\"Processing {dataset}...\")\n",
        "      data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n",
        "      #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n",
        "      data = data.select(range(10))\n",
        "      for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n",
        "          # Extract document text\n",
        "          print(f\"~~~~~~>>>  question: {row.get('question', '')}\")\n",
        "          doc_text = row.get('documents', '')\n",
        "\n",
        "          # Skip if no documents found\n",
        "          if not doc_text:\n",
        "              continue\n",
        "\n",
        "          # Process the document\n",
        "          processed_output = process_document_with_identifiers(doc_text)\n",
        "          added_item_idxs = set()\n",
        "\n",
        "          # Populate the lists\n",
        "          for section_idx, section in enumerate(processed_output):\n",
        "              for item_idx, (prefix, content) in enumerate(section):\n",
        "                  # Skip if this item_idx has already been processed\n",
        "                  if item_idx in added_item_idxs:\n",
        "                      continue\n",
        "\n",
        "                  # Add the item_idx to the set to track it\n",
        "                  added_item_idxs.add(item_idx)\n",
        "\n",
        "                  # Add the document\n",
        "                  document = f\"[{prefix}] {content}\"\n",
        "                  all_documents.append(document)\n",
        "\n",
        "                  # Construct a globally unique ID\n",
        "                  doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n",
        "                  all_ids.append(doc_id)\n",
        "\n",
        "                  # Construct metadata\n",
        "                  metadata = {\n",
        "                      \"dataset\": dataset,\n",
        "                      \"global_index\": doc_idx,\n",
        "                      \"section_index\": section_idx,\n",
        "                      \"item_index\": item_idx,\n",
        "                      \"prefix\": prefix,\n",
        "                      \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n",
        "                  }\n",
        "                  all_metadatas.append(metadata)\n",
        "\n",
        "          doc_idx += 1  # Increment global document index\n",
        "\n",
        "  # Step 4: Generate Embeddings\n",
        "  embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer\n",
        "  batch_size = 2500  # Adjust based on available memory\n",
        "\n",
        "  # Generate embeddings in batches\n",
        "  all_embeddings = []\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n",
        "      batch_docs = all_documents[i:i + batch_size]\n",
        "      batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n",
        "      all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "  bm25_encoder = prepare_bm25_encoder(all_documents)\n",
        "  sparse_vectors = [generate_sparse_vector_bm25(text, bm25_encoder) for text in all_documents]\n",
        "\n",
        "  # Adding data to milvus with enhanced duplicate check\n",
        "  all_recs = datastor.get_all_records(milvus_collection_name)\n",
        "  #print(f\"sample: {str(all_recs[0])}\")\n",
        "  existing_hashes = get_existing_hashes_milvus(all_recs)\n",
        "\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to DB\"):\n",
        "      batch_embeddings = all_embeddings[i:i + batch_size]\n",
        "      batch_sparse_embs = sparse_vectors[i:i + batch_size]\n",
        "      batch_metadatas = all_metadatas[i:i + batch_size]\n",
        "      batch_documents = all_documents[i:i + batch_size]\n",
        "      batch_ids = []\n",
        "\n",
        "      # Generate hashes for each document in the batch\n",
        "      for doc, metadata in zip(batch_documents, batch_metadatas):\n",
        "          doc_hash = generate_hash(doc, metadata)\n",
        "          if doc_hash not in existing_hashes:\n",
        "              batch_ids.append(doc_hash)\n",
        "              existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n",
        "          else:\n",
        "              print(f\"Skipping duplicate document: {doc[:15]}...\")  # Print a preview of the duplicate doc\n",
        "\n",
        "      # Add non-duplicate documents to the database\n",
        "      if batch_ids:  # Ensure there are non-duplicate documents to add\n",
        "          # Add the batch to the Milvus collection\n",
        "          if store_client == \"Milvus\" and insert_data:\n",
        "              datastor.insert(milvus_collection_name,\n",
        "                  metadata=batch_metadatas,\n",
        "                  documents=batch_documents,\n",
        "                  sparse_embs = np.array(batch_sparse_embs),\n",
        "                  embeddings=np.array(batch_embeddings),\n",
        "                  ids=batch_ids\n",
        "              )\n",
        "\n",
        "  print(f\"total data inserted into {milvus_collection_name} iteration {i}\")\n",
        "  print(f\"records : {datastor.has_entities(milvus_collection_name)} \")"
      ],
      "metadata": {
        "id": "7V650exmiLNE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kLJFIJ8j4USZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n",
        "\n",
        "!cp /content/drive/MyDrive/ragbench.db /content/ragbench.db\n",
        "\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nDac08_w6TfL",
        "outputId": "6b6186a8-07d8-48bc-fd1d-aace00b65840"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 92540\n",
            "drwx------ 7 root root     4096 Feb 15 10:18 drive\n",
            "-rw------- 1 root root 94752768 Feb 15 10:33 ragbench.db\n",
            "drwxr-xr-x 1 root root     4096 Jan  6 14:19 sample_data\n",
            "total 92540\n",
            "drwx------ 7 root root     4096 Feb 15 10:18 drive\n",
            "-rw------- 1 root root 94752768 Feb 15 10:33 ragbench.db\n",
            "drwxr-xr-x 1 root root     4096 Jan  6 14:19 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Retrieval part**"
      ],
      "metadata": {
        "id": "pA8fjaZHHJp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import langchain\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Function to retrieve relevant chunks\n",
        "def retrieve_docs_milvus(question, collection_name, top_k=10):\n",
        "    # Generate embedding for the query\n",
        "    # Generate the dense embedding for the question\n",
        "    start_time = time.time()\n",
        "    dense_query_embedding = embedder.encode(question).tolist()\n",
        "\n",
        "    # Generate the sparse embedding for the question\n",
        "    #sparse_query_embedding = generate_sparse_vector_bm25(question, bm25_encoder)\n",
        "\n",
        "    # Perform vector search to find relevant chunks\n",
        "    #results = datastor.extract_documents(datastor.search(query_embedding, milvus_collection_name, top_k))\n",
        "    results = datastor.search(dense_query_embedding, collection_name, top_k)\n",
        "    #results = datastor.hybrid_search(sparse_query_embedding, dense_query_embedding, milvus_collection_name, top_k)\n",
        "    print(f\"results: retrieve_docs_milvus >>>  {results}\")\n",
        "\n",
        "    # HyDE search with pseudo document\n",
        "    #pseudo_docs = fetch_docs_pseudo(query)\n",
        "    #print(f\"pseudo_docs: retrieve_docs_milvus >>>  {pseudo_docs}\")\n",
        "\n",
        "    # Extract 'documents' field\n",
        "    documents_list = [item['documents'] for item in results]\n",
        "    #documents_list += pseudo_docs\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Hybrid Search completed. Found {len(results)} results (approx. 2 * {top_k} + alpha * {top_k}). in {end_time - start_time} secs\")\n",
        "    # Print the extracted documents\n",
        "    print(\"retrieve_docs_milvus >>> documents_list from Hybrid + HyDE search >>>>\", documents_list)\n",
        "\n",
        "    # Extract the retrieved chunks\n",
        "    # chunks = documents_list\n",
        "    # should sort and push context - but later\n",
        "\n",
        "    return documents_list\n",
        "\n",
        "# Load Cross-Encoder model and tokenizer\n",
        "cross_encoder_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model_name)\n",
        "\n",
        "# Ensure PyTorch is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def rerank_with_cross_encoder(question):\n",
        "    \"\"\"\n",
        "    Rerank documents based on relevance scores from a Cross-Encoder model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string.\n",
        "        documents (list): A list of document strings.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (document, score), sorted by score in descending order.\n",
        "    \"\"\"\n",
        "    query_embedding = embedder.encode(question)\n",
        "    results = datastor.search(query_embedding, 5)\n",
        "    documents = datastor.extract_documents(results)\n",
        "\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        # Tokenize query-document pair\n",
        "        inputs = tokenizer(\n",
        "            question,\n",
        "            doc,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,  # Limit for most transformer models\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "        # Compute relevance scores\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Handle binary classification or regression logits\n",
        "            if logits.size(1) == 2:  # Binary classification\n",
        "                score = torch.softmax(logits, dim=1)[:, 1].item()  # Probability of relevance (class 1)\n",
        "            else:  # Regression or single-class output\n",
        "                score = logits.squeeze().item()  # Direct score (e.g., relevance regression)\n",
        "\n",
        "            scores.append((doc, score))\n",
        "    # Sort by score in descending order\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def query_response_from_llm(query: str, collection_name, model_name=\"llama3-8b-8192\"):\n",
        "\n",
        "    # retrieve chunks from milvus db\n",
        "    chunks = retrieve_docs_milvus(query, collection_name)\n",
        "\n",
        "    if not chunks:\n",
        "        return \"No relevant documents found.\", \"\"\n",
        "\n",
        "    # retrieve chunks from chroma db\n",
        "    #chunks = retrieve_docs(query)\n",
        "\n",
        "    # Flatten the list if necessary\n",
        "    if any(isinstance(chunk, list) for chunk in chunks):\n",
        "      chunks = [item for sublist in chunks for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "    chat = ChatGroq(temperature=0.3, groq_api_key=\"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\", model_name=\"llama3-8b-8192\")\n",
        "\n",
        "    #chat = ChatGroq(temperature=0.3, groq_api_key=\"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\", model_name=\"deepseek-r1-distill-llama-70b\")\n",
        "\n",
        "    prompt=ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Please provide a response to the query below, strictly adhering to the\n",
        "      information presented in the following documents.\n",
        "      Do not generate any text beyond what is explicitly stated in the documents.\n",
        "\n",
        "      Context: {context}\n",
        "\n",
        "      Question: {query}\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Summarize the retrieved doc chunks\n",
        "    # Compress documents using abstractive summarization before appending to context\n",
        "    chunk_summary = \"\"\n",
        "\n",
        "    '''\n",
        "    print(\"chunks>>>>\",chunks)\n",
        "    sorted_chunks = sort_documents(query, chunks)\n",
        "    print(f\"sorted_chunks: {sorted_chunks}\")\n",
        "    chunk_summary = summarize_docs(sorted_chunks)\n",
        "    print(\"chunk_summary>>>>\",chunk_summary)\n",
        "    chunks = chunk_summary.join(chunks)\n",
        "    '''\n",
        "\n",
        "    #Cross encoder reranked docs (comment this for summarization)\n",
        "    reranked_chunks = rerank_with_cross_encoder(query)\n",
        "    doc_chunks = [t[0] for t in reranked_chunks]\n",
        "    print(doc_chunks)\n",
        "\n",
        "    #monot5 reranked docs\n",
        "    #reranked_chunks = rerank_with_monot5(question)\n",
        "\n",
        "    #doc_chunks = [chunk[0] for chunk in reranked_chunks]\n",
        "    #chunk_summary = summarize_docs(doc_chunks)\n",
        "    #doc_chunks = chunk_summary\n",
        "\n",
        "    print(\"doc_chunks>>after re-ranking>>\", doc_chunks)\n",
        "    chain = prompt | chat\n",
        "\n",
        "    context = \"\".join(doc_chunks)\n",
        "\n",
        "    print(\"context>>>from 1st RAG>>>>>> \",context)\n",
        "\n",
        "    groq_response = chain.invoke({\"context\": context, \"query\": query})\n",
        "\n",
        "    print(\"groq_response>>>from 1st RAG>>>>>> \",groq_response)\n",
        "\n",
        "    answer = groq_response\n",
        "    return answer, context\n",
        "\n",
        "def generate_prompt():\n",
        "    \"\"\"\n",
        "    Generate a prompt template for assessing the support and relevance of an LLM-generated response.\n",
        "    \"\"\"\n",
        "    return \"\"\"\n",
        "    I asked someone to answer a question based on one or more documents.\n",
        "    Your task is to review their response and assess whether or not each sentence\n",
        "    in that response is supported by text in the documents. And if so, which\n",
        "    sentences in the documents provide that support. You will also tell me which\n",
        "    of the documents contain useful information for answering the question, and\n",
        "    which of the documents the answer was sourced from.\n",
        "    Here are the documents, each of which is split into sentences.Alongside each\n",
        "    sentence is associated key, such as [0a]. or [0b]. that you can use to refer\n",
        "    to it:\n",
        "\n",
        "    \n",
        "    {documents}\n",
        "    \n",
        "    The question was:\n",
        "    \n",
        "    {question}\n",
        "    \n",
        "\n",
        "    Here is their response, split into sentences. Alongside each sentence is\n",
        "    associated key, such as a. or b. that you can use to refer to it. Note\n",
        "    that these keys are unique to the response, and are not related to the keys\n",
        "    in the documents:\n",
        "    \n",
        "    {answer}\n",
        "    \n",
        "    You must respond with a JSON object matching this schema:\n",
        "    \n",
        "    {{\n",
        "    \"relevance_explanation\": string,\n",
        "    \"all_relevant_sentence_keys\": [string],\n",
        "    \"overall_supported_explanation\": string,\n",
        "    \"overall_supported\": boolean,\n",
        "    \"sentence_support_information\": [\n",
        "    {{\n",
        "    \"response_sentence_key\": string,\n",
        "    \"explanation\": string,\n",
        "    \"supporting_sentence_keys\": [string],\n",
        "    \"fully_supported\": boolean\n",
        "    }},\n",
        "    ],\n",
        "    \"all_utilized_sentence_keys\": [string]\n",
        "    }}\n",
        "    \n",
        "    The relevance_explanation field is a string explaining which documents\n",
        "    contain useful information for answering the question. Provide a step-by-step\n",
        "    breakdown of information provided in the documents and how it is useful for\n",
        "    answering the question.\n",
        "    The all_relevant_sentence_keys field is a list of all document sentences keys\n",
        "    (e.g. 0a) that are relevant to the question. Include every sentence that is\n",
        "    useful and relevant to the question, even if it was not used in the response,\n",
        "    or if only parts of the sentence are useful. Ignore the provided response when\n",
        "    making this judgement and base your judgement solely on the provided documents\n",
        "    and question. Omit sentences that, if removed from the document, would not\n",
        "    impact someones ability to answer the question.\n",
        "    The overall_supported_explanation field is a string explaining why the response\n",
        "    *as a whole* is or is not supported by the documents. In this field, provide a\n",
        "    step-by-step breakdown of the claims made in the response and the support (or\n",
        "    lack thereof) for those claims in the documents. Begin by assessing each claim\n",
        "    separately, one by one; dont make any remarks about the response as a whole\n",
        "    until you have assessed all the claims in isolation.\n",
        "    The overall_supported field is a boolean indicating whether the response as a\n",
        "    whole is supported by the documents. This value should reflect the conclusion\n",
        "    you drew at the end of your step-by-step breakdown in overall_supported_explanation.\n",
        "    In the sentence_support_information field, provide information about the support\n",
        "    *for each sentence* in the response.\n",
        "    The sentence_support_information field is a list of objects, one for each sentence\n",
        "    in the response. Each object MUST have the following fields:\n",
        "    - response_sentence_key: a string identifying the sentence in the response.\n",
        "    This key is the same as the one used in the response above.\n",
        "\n",
        "    - explanation: a string explaining why the sentence is or is not supported by the\n",
        "    documents.\n",
        "    - supporting_sentence_keys: keys (e.g. [0a]) of sentences from the documents that\n",
        "    support the response sentence. If the sentence is not supported, this list MUST\n",
        "    be empty. If the sentence is supported, this list MUST contain one or more keys.\n",
        "    In special cases where the sentence is supported, but not by any specific sentence,\n",
        "    you can use the string \"supported_without_sentence\" to indicate that the sentence\n",
        "    is generally supported by the documents. Consider cases where the sentence is\n",
        "    expressing inability to answer the question due to lack of relevant information in\n",
        "    the provided context as \"supported_without_sentence\". In cases where the sentence\n",
        "    is making a general statement (e.g. outlining the steps to produce an answer, or\n",
        "    summarizing previously stated sentences, or a transition sentence), use the\n",
        "    string \"general\". In cases where the sentence is correctly stating a well-known fact,\n",
        "    like a mathematical formula, use the string \"well_known_fact\". In cases where the\n",
        "    sentence is performing numerical reasoning (e.g. addition, multiplication), use\n",
        "    the string \"numerical_reasoning\".\n",
        "    - fully_supported: a boolean indicating whether the sentence is fully supported by\n",
        "    the documents.\n",
        "    - This value should reflect the conclusion you drew at the end of your step-by-step\n",
        "    breakdown in explanation.\n",
        "    - If supporting_sentence_keys is an empty list, then fully_supported must be false.\n",
        "    - Otherwise, use fully_supported to clarify whether everything in the response\n",
        "    sentence is fully supported by the document text indicated in supporting_sentence_keys\n",
        "    (fully_supported = true), or whether the sentence is only partially or incompletely\n",
        "    supported by that document text (fully_supported = false).\n",
        "    The all_utilized_sentence_keys field is a list of all sentences keys (e.g. 0a) that\n",
        "    were used to construct the answer. Include every sentence that either directly supported\n",
        "    the answer, or was implicitly used to construct the answer, even if it was not used\n",
        "    in its entirety. Omit sentences that were not used, and could have been removed from\n",
        "    the documents without affecting the answer.\n",
        "    You must respond with a valid JSON string. Use escapes for quotes, e.g. \\\\\"\\\\\", and\n",
        "    newlines, e.g. \\\\n. Do not write anything before or after the JSON string. Do not\n",
        "    wrap the JSON string in backticks like  or json.\n",
        "    As a reminder: your task is to review the response and assess which documents contain\n",
        "    useful information pertaining to the question, and how each sentence in the response\n",
        "    is supported by the text in the documents.\n",
        "    \"\"\".strip()\n",
        "\n",
        "def analyze_llm_response(question, context, answer):\n",
        "  chat = ChatGroq(temperature=0.3, groq_api_key=\"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\", model_name=\"llama3-70b-8192\")\n",
        "\n",
        "  prompt_template_with_docs = PromptTemplate(\n",
        "      input_variables=[\"documents\", \"question\", \"answer\"],\n",
        "      template=generate_prompt(),\n",
        "  )\n",
        "\n",
        "  chain = prompt_template_with_docs | chat\n",
        "  groq_response_with_context_qanda = chain.invoke({\"documents\": context, \"question\": question, \"answer\":answer})\n",
        "  return groq_response_with_context_qanda.content"
      ],
      "metadata": {
        "id": "vsitelPOYdh1"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0_SylVhlMx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import gradio as gr\n",
        "\n",
        "# Placeholder function to get response from your LLM/RAG pipeline\n",
        "def get_response(query, model_name, rag_bench):\n",
        "    collection_name = f\"ragbench_collection_{rag_bench}\"\n",
        "    print(f\"Response from {collection_name} for query: {query}\")  # Replace with actual model call\n",
        "    answer, context = query_response_from_llm(query, rag_bench, model_name)\n",
        "    if not context:\n",
        "      return answer, \"{}\"\n",
        "    response = analyze_llm_response(query, context, answer)\n",
        "    analytics_json = json.dumps(response, indent=2)  # Pretty JSON format\n",
        "    return answer, analytics_json\n",
        "\n",
        "# Available LLM models\n",
        "llm_models = [\"llama3-8b-8192\", \"deepseek-r1-distill-llama-70b\"]  # Modify as needed\n",
        "rag_banch_datasets = ['covidqa','cuad','delucionqa','emanual','expertqa',\n",
        "                      'finqa','hagrid','hotpotqa','msmarco','pubmedqa','tatqa','techqa']\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## LLM/RAG Query Interface\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Enter your query\")\n",
        "        rag_bench_selector = gr.Dropdown(rag_banch_datasets, label=\"Select RAG Bench Dataset\")\n",
        "        model_selector = gr.Dropdown(llm_models, label=\"Select LLM Model\")\n",
        "\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    answer = gr.Textbox(label=\"LLM Answer\", interactive=False)\n",
        "    json = gr.Code(label=\"Analysis Output\", interactive=False, language=\"json\")\n",
        "\n",
        "    submit_btn.click(get_response, inputs=[query_input, model_selector, rag_bench_selector], outputs=(answer, json))\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "53IKsl8l6ejg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "bc6e0e92-6d9e-4dc7-f85f-b73c8f758d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://51d7892868085b9913.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://51d7892868085b9913.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from ragbench_collection_covidqa for query: What is MVO?\n"
          ]
        }
      ]
    }
  ]
}