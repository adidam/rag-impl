{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidam/rag-impl/blob/main/rag_visualizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjIAeZQ6PZD",
        "outputId": "6d28f9c2-d08e-4745-fd17-8d7c04693492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.16.0-py3-none-any.whl.metadata (16 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install pymilvus pymilvus[model]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NIN5yZ2mhjoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and other utility functions**"
      ],
      "metadata": {
        "id": "7YUOuy1bjA4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_model = \"BAAI/LLM-Embedder\"\n",
        "# embedding_model = \"BAAI/bge-large-en\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer"
      ],
      "metadata": {
        "id": "qB-77iRMj4t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New code - 12/4 10 pm\n",
        "import hashlib\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sliding window configuration\n",
        "TOKEN_LIMIT = 512\n",
        "SLIDING_WINDOW_OVERLAP = 100  # Overlap between consecutive chunks (in tokens)\n",
        "\n",
        "# Function for chunking with token limit and sliding window\n",
        "def chunk_with_token_limit(text, token_limit=512, overlap=100):\n",
        "    sentences = sent_tokenize(text)  # Split text into sentences\n",
        "    chunks = []  # Store resulting chunks\n",
        "    current_chunk = []  # Temporarily hold sentences for the current chunk\n",
        "    current_chunk_tokens = 0  # Token count for the current chunk\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence and calculate its token count\n",
        "        sentence_tokens = tokenizer.tokenize(sentence)\n",
        "        num_tokens = len(sentence_tokens)\n",
        "\n",
        "        # print(f\"Tokens: {sentence_tokens[0]}\")\n",
        "\n",
        "        # If adding this sentence exceeds the token limit\n",
        "        if current_chunk_tokens + num_tokens > token_limit:\n",
        "            # Save the current chunk\n",
        "            chunk_text = \" \".join(current_chunk)\n",
        "            chunks.append(chunk_text)\n",
        "\n",
        "            # Prepare the next chunk with overlap\n",
        "            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n",
        "            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n",
        "            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_chunk_tokens += num_tokens\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk:\n",
        "        chunk_text = \" \".join(current_chunk)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_document_with_identifiers(document):\n",
        "    processed_data = []\n",
        "    title_count = -1  # to start from 0\n",
        "    # print(\"document>>>>>>>\",document)\n",
        "    for section in document:\n",
        "        section_chunks = []\n",
        "        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n",
        "        title_count += 1  # Increment title count\n",
        "\n",
        "        # Tokenize the section into sentences\n",
        "        sentences = sent_tokenize(section)\n",
        "        for sentence in sentences:\n",
        "            if sentence.startswith(\"Title:\"):\n",
        "                # New document detected\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "                passage_count = [ord('a')]  # Reset passage count for the new document\n",
        "            else:\n",
        "                # Sentence under the current document\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "                print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "\n",
        "                # Increment passage_count intelligently\n",
        "                i = len(passage_count) - 1\n",
        "                while i >= 0:\n",
        "                    passage_count[i] += 1\n",
        "                    if passage_count[i] > ord('z'):\n",
        "                        passage_count[i] = ord('a')\n",
        "                        if i == 0:\n",
        "                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n",
        "                        i -= 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "\n",
        "        print(\"section_chunks>>>>>>>\",section_chunks)\n",
        "        processed_data.append(section_chunks)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Function to generate a hash based on content and key metadata\n",
        "def generate_hash(content, metadata):\n",
        "    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n",
        "    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n",
        "    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes(collection):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n",
        "    existing_hashes = set()\n",
        "    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes_milvus(all_records):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    existing_hashes = set()\n",
        "    print(f\"all records >>> {len(all_records)}\")\n",
        "    if all_records == None or len(all_records) == 0:\n",
        "        return existing_hashes\n",
        "\n",
        "    for record in all_records:\n",
        "        doc = record.get(\"documents\")\n",
        "        metadata = record.get(\"metadata\")\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "\n",
        "# Tokenize corpus and prepare BM25 encoder\n",
        "def prepare_bm25_encoder(texts):\n",
        "    tokenized_corpus = [word_tokenize(text.lower()) for text in texts]\n",
        "    bm25_encoder = BM25Okapi(tokenized_corpus)\n",
        "    return bm25_encoder\n",
        "\n",
        "def generate_sparse_vector_bm25(query, bm25_encoder):\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    scores = bm25_encoder.get_scores(tokenized_query)\n",
        "    # Convert scores to CSR format\n",
        "    sparse_vector = sp.csr_matrix(scores)\n",
        "    return sparse_vector"
      ],
      "metadata": {
        "id": "pK2czU0-hoHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milvus VectorDataStore class**"
      ],
      "metadata": {
        "id": "J5QuDMffi2K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pymilvus import connections\n",
        "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
        "from pymilvus import MilvusClient\n",
        "from pymilvus import utility\n",
        "\n",
        "class VectorDataStore:\n",
        "    db_url = \"http://localhost:19530\"\n",
        "\n",
        "    #description = f\"collection created for {self.name}\"\n",
        "\n",
        "    def __init__(self, path=\"/content/ragbench.db\"):\n",
        "        self.client = MilvusClient(path)\n",
        "\n",
        "\n",
        "    def get_or_create_collection(self, name, vec_dim=128):\n",
        "        try:\n",
        "            self.get_collection(name)\n",
        "        except:\n",
        "            print(f\"Collection {name} doesn't exist. Creating...\")\n",
        "            self.create_collection(name, vec_dim)\n",
        "\n",
        "\n",
        "    def create_collection(self, name, vec_dim=128):\n",
        "        if self.client.has_collection(name):\n",
        "            self.default_collection_name = name\n",
        "\n",
        "        self.description = f\"collection to store {name}\"\n",
        "\n",
        "        index_params = self.client.prepare_index_params()\n",
        "        index_params.add_index(\n",
        "            field_name=\"embedding\",\n",
        "            index_type=\"AUTOINDEX\",\n",
        "            metric_type=\"COSINE\"\n",
        "        )\n",
        "        index_params.add_index(\n",
        "            field_name=\"sparse\",\n",
        "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
        "            metric_type=\"IP\"\n",
        "        )\n",
        "        schema = self.client.create_schema(\n",
        "            auto_id=False,\n",
        "            enable_dynamic_fields=True,\n",
        "        )\n",
        "        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n",
        "        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
        "        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n",
        "        schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
        "        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n",
        "\n",
        "        collection = self.client.create_collection(collection_name=name,\n",
        "                                       schema=schema,\n",
        "                                       index_params=index_params)\n",
        "        self.current_collection = collection\n",
        "        return collection\n",
        "\n",
        "\n",
        "    def get_collection(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exist.\")\n",
        "        self.current_collection = Collection(name)\n",
        "        return self.current_collection\n",
        "\n",
        "    def get_all_records(self, collection):\n",
        "        all_records = self.client.query(\n",
        "            collection_name=collection,\n",
        "            filter=None,\n",
        "            output_fields=[\"documents\", \"metadata\"],\n",
        "            limit=10000\n",
        "        )\n",
        "        if all_records == None:\n",
        "            all_records = []\n",
        "\n",
        "        return all_records\n",
        "\n",
        "    def has_entities(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exists.\")\n",
        "        self.default_collection = name\n",
        "        collection_stats = self.client.get_collection_stats(collection_name)\n",
        "        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n",
        "        return count\n",
        "\n",
        "    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n",
        "                documents: list[str], sparse_embs: np.ndarray, embeddings: np.ndarray, ids: list[int]):\n",
        "\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n",
        "\n",
        "        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n",
        "           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n",
        "\n",
        "        data = []\n",
        "        for meta, doc, sp_embs, emb, id in zip(metadata, documents, sparse_embs, embeddings, ids):\n",
        "          datum = {\n",
        "              \"pk\": id,\n",
        "              \"metadata\": meta,\n",
        "              \"documents\": doc,\n",
        "              \"sparse\": sp_embs,\n",
        "              \"embedding\": emb.tolist(),\n",
        "          }\n",
        "          data.append(datum)\n",
        "\n",
        "        self.client.insert(collection_name, data)\n",
        "        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n",
        "\n",
        "    def drop_collection(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.drop_collection(collection_name)\n",
        "        print(f\"Dropped collection '{collection_name}'.\")\n",
        "\n",
        "    def delete_all(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.delete(collection_name, expr=\"pk >= 0\")\n",
        "        self.client.flush([collection_name])\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, collection='all', top_k: int = 10) -> list[dict[str, any]]:\n",
        "        \"\"\"\n",
        "        Search across all collections for the top-k closest embeddings.\n",
        "        :param query_embedding: The embedding vector to search for.\n",
        "        :param top_k: Number of top results to retrieve.\n",
        "        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        if collection=='all':\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"embedding\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def sparse_search(self, query_embedding: np.ndarray, collection='all', top_k : int=10)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        if (collection=='all')\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"IP\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"sparse\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def hybrid_search(self, sparse_query_embedding: np.ndarray, dense_query_embedding: np.ndarray, collection='all', top_k : int=10, alpha=0.3)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "        sparse_results = self.sparse_search(sparse_query_embedding, collection, top_k)\n",
        "        n = int(len(sparse_results) * alpha)\n",
        "        alpha_sparse_results = sparse_results[:n]\n",
        "        dense_results = self.search(dense_query_embedding, collection, top_k)\n",
        "        #'results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        results = dense_results + alpha_sparse_results\n",
        "        end_time = time.time()\n",
        "        print(f\"Hybrid Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n",
        "      \"\"\"\n",
        "      Extract embedding values from search results.\n",
        "      :param search_results: List of dictionaries containing search results.\n",
        "      :return: List of embedding vectors as NumPy arrays.\n",
        "      \"\"\"\n",
        "      return [result[\"documents\"] for result in search_results if \"documents\" in result]"
      ],
      "metadata": {
        "id": "5wJcp7qOieKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "milvus_collection_name = f\"ragbench_collection_{dataset}_v16\"\n",
        "datastor.drop_collection(milvus_collection_name)"
      ],
      "metadata": {
        "id": "yymOe-M_jMc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['covidqa','cuad','delucionqa','emanual','expertqa','finqa','hagrid','hotpotqa','msmarco','pubmedqa','tatqa','techqa']\n",
        "g_data_index = random.randint(1, 12) - 1\n",
        "print(f\"Selected dataset: {datasets[g_data_index]}\")\n",
        "\n",
        "\n",
        "data = load_dataset(\"rungalileo/ragbench\", datasets[g_data_index], split=\"train\")\n",
        "top_5_rows = data.select(range(5))"
      ],
      "metadata": {
        "id": "pwWHuni4h-Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbsFZFWnjmlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datastor = VectorDataStore()\n",
        "\n",
        "insert_data = True\n",
        "store_client = \"Milvus\"\n",
        "num_records = 0\n",
        "\n",
        "vector_dim = embedder.get_sentence_embedding_dimension()\n",
        "\n"
      ],
      "metadata": {
        "id": "3yv-6mPRkQ2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Initialize storage for documents, IDs, and metadata\n",
        "\n",
        "for i in range(12):\n",
        "  all_documents = []\n",
        "  all_ids = []\n",
        "  all_metadatas = []\n",
        "\n",
        "  # Process each dataset\n",
        "  doc_idx = 0  # Global document index for unique IDs\n",
        "  for dataset in [datasets[i]]:\n",
        "      milvus_collection_name = f\"ragbench_collection_{dataset}_v16\"\n",
        "      num_records = datastor.has_entities(milvus_collection_name)\n",
        "      if num_records > 0:\n",
        "          datastor.drop_collection(milvus_collection_name)\n",
        "      datastor.get_or_create_collection(milvus_collection_name, vector_dim)\n",
        "\n",
        "      print(f\"Processing {dataset}...\")\n",
        "      data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n",
        "      #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n",
        "      data = data.select(range(10))\n",
        "      for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n",
        "          # Extract document text\n",
        "          print(f\"~~~~~~>>>  question: {row.get('question', '')}\")\n",
        "          doc_text = row.get('documents', '')\n",
        "\n",
        "          # Skip if no documents found\n",
        "          if not doc_text:\n",
        "              continue\n",
        "\n",
        "          # Process the document\n",
        "          processed_output = process_document_with_identifiers(doc_text)\n",
        "          added_item_idxs = set()\n",
        "\n",
        "          # Populate the lists\n",
        "          for section_idx, section in enumerate(processed_output):\n",
        "              for item_idx, (prefix, content) in enumerate(section):\n",
        "                  # Skip if this item_idx has already been processed\n",
        "                  if item_idx in added_item_idxs:\n",
        "                      continue\n",
        "\n",
        "                  # Add the item_idx to the set to track it\n",
        "                  added_item_idxs.add(item_idx)\n",
        "\n",
        "                  # Add the document\n",
        "                  document = f\"[{prefix}] {content}\"\n",
        "                  all_documents.append(document)\n",
        "\n",
        "                  # Construct a globally unique ID\n",
        "                  doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n",
        "                  all_ids.append(doc_id)\n",
        "\n",
        "                  # Construct metadata\n",
        "                  metadata = {\n",
        "                      \"dataset\": dataset,\n",
        "                      \"global_index\": doc_idx,\n",
        "                      \"section_index\": section_idx,\n",
        "                      \"item_index\": item_idx,\n",
        "                      \"prefix\": prefix,\n",
        "                      \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n",
        "                  }\n",
        "                  all_metadatas.append(metadata)\n",
        "\n",
        "          doc_idx += 1  # Increment global document index\n",
        "\n",
        "  # Step 4: Generate Embeddings\n",
        "  embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer\n",
        "  batch_size = 2500  # Adjust based on available memory\n",
        "\n",
        "  # Generate embeddings in batches\n",
        "  all_embeddings = []\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n",
        "      batch_docs = all_documents[i:i + batch_size]\n",
        "      batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n",
        "      all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "  bm25_encoder = prepare_bm25_encoder(all_documents)\n",
        "  sparse_vectors = [generate_sparse_vector_bm25(text, bm25_encoder) for text in all_documents]\n",
        "\n",
        "  # Adding data to milvus with enhanced duplicate check\n",
        "  all_recs = datastor.get_all_records(milvus_collection_name)\n",
        "  #print(f\"sample: {str(all_recs[0])}\")\n",
        "  existing_hashes = get_existing_hashes_milvus(all_recs)\n",
        "\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to DB\"):\n",
        "      batch_embeddings = all_embeddings[i:i + batch_size]\n",
        "      batch_sparse_embs = sparse_vectors[i:i + batch_size]\n",
        "      batch_metadatas = all_metadatas[i:i + batch_size]\n",
        "      batch_documents = all_documents[i:i + batch_size]\n",
        "      batch_ids = []\n",
        "\n",
        "      # Generate hashes for each document in the batch\n",
        "      for doc, metadata in zip(batch_documents, batch_metadatas):\n",
        "          doc_hash = generate_hash(doc, metadata)\n",
        "          if doc_hash not in existing_hashes:\n",
        "              batch_ids.append(doc_hash)\n",
        "              existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n",
        "          else:\n",
        "              print(f\"Skipping duplicate document: {doc[:15]}...\")  # Print a preview of the duplicate doc\n",
        "\n",
        "      # Add non-duplicate documents to the database\n",
        "      if batch_ids:  # Ensure there are non-duplicate documents to add\n",
        "          # Add the batch to the Milvus collection\n",
        "          if store_client == \"Milvus\" and insert_data:\n",
        "              datastor.insert(milvus_collection_name,\n",
        "                  metadata=batch_metadatas,\n",
        "                  documents=batch_documents,\n",
        "                  sparse_embs = np.array(batch_sparse_embs),\n",
        "                  embeddings=np.array(batch_embeddings),\n",
        "                  ids=batch_ids\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "  print(f\"total data inserted into {milvus_collection_name} : {datastor.has_entities(milvus_collection_name)}\")"
      ],
      "metadata": {
        "id": "7V650exmiLNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7peuqwYlFok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0_SylVhlMx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Placeholder function to get response from your LLM/RAG pipeline\n",
        "def get_response(query, model_name):\n",
        "    response = f\"Response from {model_name} for query: {query}\"  # Replace with actual model call\n",
        "    return response\n",
        "\n",
        "# Available LLM models\n",
        "llm_models = [\"Llama-3.2-1B\", \"GPT-4\", \"Mistral\", \"Custom-LLM\"]  # Modify as needed\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## LLM/RAG Query Interface\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Enter your query\")\n",
        "        model_selector = gr.Dropdown(llm_models, label=\"Select LLM Model\")\n",
        "\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    response_output = gr.Textbox(label=\"Response\", interactive=False)\n",
        "\n",
        "    submit_btn.click(get_response, inputs=[query_input, model_selector], outputs=response_output)\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "53IKsl8l6ejg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}