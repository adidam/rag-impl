{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3385458a0ac4eb0be54260e74f97a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63961e70a0ed449381a92e54d3aae1d8",
              "IPY_MODEL_79507fb57fd1457b849e0e411e5b94e4",
              "IPY_MODEL_9cacdba6588d4f1e8e37442fbf2d2bcd"
            ],
            "layout": "IPY_MODEL_bc86b38342ae4738a4d878761f7eeebd"
          }
        },
        "63961e70a0ed449381a92e54d3aae1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ecfe8ab37e4986a24040408556c023",
            "placeholder": "​",
            "style": "IPY_MODEL_f57a5e1c0ba14bb9a27bddcafffa4fc7",
            "value": "Batches: 100%"
          }
        },
        "79507fb57fd1457b849e0e411e5b94e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3af500badf746eda9dec18ce4209bc5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed42257f7c214553a724154dc6de74cf",
            "value": 2
          }
        },
        "9cacdba6588d4f1e8e37442fbf2d2bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da21a2272efd4176b2f6b8a3ecb26112",
            "placeholder": "​",
            "style": "IPY_MODEL_acd74cfad584461fbb5bc15ad78dbaee",
            "value": " 2/2 [00:20&lt;00:00,  8.98s/it]"
          }
        },
        "bc86b38342ae4738a4d878761f7eeebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ecfe8ab37e4986a24040408556c023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57a5e1c0ba14bb9a27bddcafffa4fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3af500badf746eda9dec18ce4209bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed42257f7c214553a724154dc6de74cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da21a2272efd4176b2f6b8a3ecb26112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd74cfad584461fbb5bc15ad78dbaee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidam/rag-impl/blob/main/rag_visualizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjIAeZQ6PZD",
        "outputId": "5b5a20ad-766a-40da-926a-45de380e4ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.16.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (4.25.6)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: milvus-model>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus[model]) (0.2.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.48.2)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.20.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.4->pymilvus) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.5.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (4.12.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!pip install datasets\n",
        "!pip install gradio\n",
        "!pip install pymilvus pymilvus[model]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "NIN5yZ2mhjoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6670ad6d-efd9-422d-9895-24bd37a7cf5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and other utility functions**"
      ],
      "metadata": {
        "id": "7YUOuy1bjA4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_model = \"BAAI/LLM-Embedder\"\n",
        "# embedding_model = \"BAAI/bge-large-en\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer"
      ],
      "metadata": {
        "id": "qB-77iRMj4t9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9958e182-ecf5-4435-edfd-3fd6c53c0407"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New code - 12/4 10 pm\n",
        "import hashlib\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sliding window configuration\n",
        "TOKEN_LIMIT = 512\n",
        "SLIDING_WINDOW_OVERLAP = 100  # Overlap between consecutive chunks (in tokens)\n",
        "\n",
        "# Function for chunking with token limit and sliding window\n",
        "def chunk_with_token_limit(text, token_limit=512, overlap=100):\n",
        "    sentences = sent_tokenize(text)  # Split text into sentences\n",
        "    chunks = []  # Store resulting chunks\n",
        "    current_chunk = []  # Temporarily hold sentences for the current chunk\n",
        "    current_chunk_tokens = 0  # Token count for the current chunk\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence and calculate its token count\n",
        "        sentence_tokens = tokenizer.tokenize(sentence)\n",
        "        num_tokens = len(sentence_tokens)\n",
        "\n",
        "        # print(f\"Tokens: {sentence_tokens[0]}\")\n",
        "\n",
        "        # If adding this sentence exceeds the token limit\n",
        "        if current_chunk_tokens + num_tokens > token_limit:\n",
        "            # Save the current chunk\n",
        "            chunk_text = \" \".join(current_chunk)\n",
        "            chunks.append(chunk_text)\n",
        "\n",
        "            # Prepare the next chunk with overlap\n",
        "            overlap_tokens = tokenizer.tokenize(\" \".join(current_chunk[-1:]))\n",
        "            current_chunk = [sentence for sentence in current_chunk[-(overlap // len(overlap_tokens)) :]] if current_chunk else []\n",
        "            current_chunk_tokens = sum(len(tokenizer.tokenize(sent)) for sent in current_chunk)\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_chunk_tokens += num_tokens\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk:\n",
        "        chunk_text = \" \".join(current_chunk)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_document_with_identifiers(document):\n",
        "    processed_data = []\n",
        "    title_count = -1  # to start from 0\n",
        "    # print(\"document>>>>>>>\",document)\n",
        "    for section in document:\n",
        "        section_chunks = []\n",
        "        passage_count = [ord('a')]  # Passage identifier as a list to handle nested increments\n",
        "        title_count += 1  # Increment title count\n",
        "\n",
        "        # Tokenize the section into sentences\n",
        "        sentences = sent_tokenize(section)\n",
        "        for sentence in sentences:\n",
        "            if sentence.startswith(\"Title:\"):\n",
        "                # New document detected\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"  # Identifier for the title\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "                passage_count = [ord('a')]  # Reset passage count for the new document\n",
        "            else:\n",
        "                # Sentence under the current document\n",
        "                identifier = f\"{title_count}{''.join(chr(c) for c in passage_count)}\"\n",
        "\n",
        "                # Commented this line to integrate and test with small to big. To be uncommented after testing\n",
        "                chunked_texts = chunk_with_token_limit(sentence, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "\n",
        "                #Added this line to replace sliding window tokenization with hybrid tokenization (sliding window + small-to-big)\n",
        "                #chunked_texts = hybrid_chunking(sentence,tokenizer, TOKEN_LIMIT, SLIDING_WINDOW_OVERLAP)\n",
        "                print(\"chunked_texts>>>>process_document_with_identifiers>>>>> \"+ \"\".join(chunked_texts))\n",
        "\n",
        "                for chunk in chunked_texts:\n",
        "                    section_chunks.append([identifier, chunk])\n",
        "\n",
        "                # Increment passage_count intelligently\n",
        "                i = len(passage_count) - 1\n",
        "                while i >= 0:\n",
        "                    passage_count[i] += 1\n",
        "                    if passage_count[i] > ord('z'):\n",
        "                        passage_count[i] = ord('a')\n",
        "                        if i == 0:\n",
        "                            passage_count.insert(0, ord('a'))  # Add a new character to the identifier\n",
        "                        i -= 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "\n",
        "        print(\"section_chunks>>>>>>>\",section_chunks)\n",
        "        processed_data.append(section_chunks)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Function to generate a hash based on content and key metadata\n",
        "def generate_hash(content, metadata):\n",
        "    \"\"\"Generate a unique hash for the document content and key metadata.\"\"\"\n",
        "    key_fields = f\"{content}|{metadata.get('item_index')}|{metadata.get('prefix')}\"\n",
        "    return hashlib.md5(key_fields.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes(collection):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    all_records = collection.get(include=[\"documents\", \"metadatas\"])  # Fetch documents and metadata\n",
        "    existing_hashes = set()\n",
        "    for doc, metadata in zip(all_records[\"documents\"], all_records[\"metadatas\"]):\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "# Function to retrieve existing hashes from the database\n",
        "def get_existing_hashes_milvus(all_records):\n",
        "    \"\"\"Retrieve all existing hashes (IDs) currently in the database.\"\"\"\n",
        "    existing_hashes = set()\n",
        "    print(f\"all records >>> {len(all_records)}\")\n",
        "    if all_records == None or len(all_records) == 0:\n",
        "        return existing_hashes\n",
        "\n",
        "    for record in all_records:\n",
        "        doc = record.get(\"documents\")\n",
        "        metadata = record.get(\"metadata\")\n",
        "        doc_hash = generate_hash(doc, metadata)\n",
        "        existing_hashes.add(doc_hash)\n",
        "    return existing_hashes\n",
        "\n",
        "\n",
        "# Tokenize corpus and prepare BM25 encoder\n",
        "def prepare_bm25_encoder(texts):\n",
        "    tokenized_corpus = [word_tokenize(text.lower()) for text in texts]\n",
        "    bm25_encoder = BM25Okapi(tokenized_corpus)\n",
        "    return bm25_encoder\n",
        "\n",
        "def generate_sparse_vector_bm25(query, bm25_encoder):\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    scores = bm25_encoder.get_scores(tokenized_query)\n",
        "    # Convert scores to CSR format\n",
        "    sparse_vector = sp.csr_matrix(scores)\n",
        "    return sparse_vector"
      ],
      "metadata": {
        "id": "pK2czU0-hoHD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milvus VectorDataStore class**"
      ],
      "metadata": {
        "id": "J5QuDMffi2K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from pymilvus import connections\n",
        "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
        "from pymilvus import MilvusClient\n",
        "from pymilvus import utility\n",
        "\n",
        "class VectorDataStore:\n",
        "    db_url = \"http://localhost:19530\"\n",
        "\n",
        "    #description = f\"collection created for {self.name}\"\n",
        "\n",
        "    def __init__(self, path=\"/content/ragbench.db\"):\n",
        "        self.client = MilvusClient(path)\n",
        "\n",
        "\n",
        "    def get_or_create_collection(self, name, vec_dim=128):\n",
        "        try:\n",
        "            self.get_collection(name)\n",
        "        except:\n",
        "            print(f\"Collection {name} doesn't exist. Creating...\")\n",
        "            self.create_collection(name, vec_dim)\n",
        "\n",
        "\n",
        "    def create_collection(self, name, vec_dim=128):\n",
        "        if self.client.has_collection(name):\n",
        "            self.default_collection_name = name\n",
        "\n",
        "        self.description = f\"collection to store {name}\"\n",
        "\n",
        "        index_params = self.client.prepare_index_params()\n",
        "        index_params.add_index(\n",
        "            field_name=\"embedding\",\n",
        "            index_type=\"AUTOINDEX\",\n",
        "            metric_type=\"COSINE\"\n",
        "        )\n",
        "        index_params.add_index(\n",
        "            field_name=\"sparse\",\n",
        "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
        "            metric_type=\"IP\"\n",
        "        )\n",
        "        schema = self.client.create_schema(\n",
        "            auto_id=False,\n",
        "            enable_dynamic_fields=True,\n",
        "        )\n",
        "        schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, max_length=64, is_primary=True)\n",
        "        schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
        "        schema.add_field(field_name=\"documents\", datatype=DataType.VARCHAR, max_length=512)\n",
        "        schema.add_field(field_name=\"sparse\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
        "        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=vec_dim)\n",
        "\n",
        "        collection = self.client.create_collection(collection_name=name,\n",
        "                                       schema=schema,\n",
        "                                       index_params=index_params)\n",
        "        self.current_collection = collection\n",
        "        return collection\n",
        "\n",
        "\n",
        "    def get_collection(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exist.\")\n",
        "        self.current_collection = Collection(name)\n",
        "        return self.current_collection\n",
        "\n",
        "    def get_all_records(self, collection):\n",
        "        all_records = self.client.query(\n",
        "            collection_name=collection,\n",
        "            filter=None,\n",
        "            output_fields=[\"documents\", \"metadata\"],\n",
        "            limit=10000\n",
        "        )\n",
        "        if all_records == None:\n",
        "            all_records = []\n",
        "\n",
        "        return all_records\n",
        "\n",
        "    def has_entities(self, name):\n",
        "        if not self.client.has_collection(name):\n",
        "            raise ValueError(f\"Collection '{name}' does not exists.\")\n",
        "        self.default_collection = name\n",
        "        collection_stats = self.client.get_collection_stats(name)\n",
        "        count = collection_stats.get(\"row_count\", 0)  # Retrieve the number of entities\n",
        "        return count\n",
        "\n",
        "    def insert(self, collection_name: str, metadata: list[dict[str, any]],\n",
        "                documents: list[str], sparse_embs: np.ndarray, embeddings: np.ndarray, ids: list[int]):\n",
        "\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist. Create it first.\")\n",
        "\n",
        "        if len(metadata) != len(embeddings) != len(documents) != len(ids):\n",
        "           raise ValueError(\"Metadata, documnets, ids and embeddings must have the same length.\")\n",
        "\n",
        "        data = []\n",
        "        for meta, doc, sp_embs, emb, id in zip(metadata, documents, sparse_embs, embeddings, ids):\n",
        "          datum = {\n",
        "              \"pk\": id,\n",
        "              \"metadata\": meta,\n",
        "              \"documents\": doc,\n",
        "              \"sparse\": sp_embs,\n",
        "              \"embedding\": emb.tolist(),\n",
        "          }\n",
        "          data.append(datum)\n",
        "\n",
        "        self.client.insert(collection_name, data)\n",
        "        print(f\"Inserted {len(metadata)} records into collection '{collection_name}'.\")\n",
        "\n",
        "    def drop_collection(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.drop_collection(collection_name)\n",
        "        print(f\"Dropped collection '{collection_name}'.\")\n",
        "\n",
        "    def delete_all(self, collection_name: str):\n",
        "        if not self.client.has_collection(collection_name):\n",
        "            raise ValueError(f\"Collection '{collection_name}' does not exist.\")\n",
        "        self.client.delete(collection_name, expr=\"pk >= 0\")\n",
        "        self.client.flush([collection_name])\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, collection='all', top_k: int = 10) -> list[dict[str, any]]:\n",
        "        \"\"\"\n",
        "        Search across all collections for the top-k closest embeddings.\n",
        "        :param query_embedding: The embedding vector to search for.\n",
        "        :param top_k: Number of top results to retrieve.\n",
        "        :return: A list of dictionaries containing collection name, id, metadata, and distance.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        if collection=='all':\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"embedding\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def sparse_search(self, query_embedding: np.ndarray, collection='all', top_k : int=10)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        if (collection=='all'):\n",
        "          collections = self.client.list_collections()\n",
        "        else:\n",
        "          collections = [collection]\n",
        "        start_time = time.time()\n",
        "        for collection_name in collections:\n",
        "            if not self.client.has_collection(collection_name):\n",
        "                continue\n",
        "\n",
        "            # Set params to COSINE to match chromadb\n",
        "            search_params = {\"metric_type\": \"IP\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "            search_results = self.client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=[query_embedding],\n",
        "                anns_field=\"sparse\",\n",
        "                search_params=search_params,\n",
        "                limit=top_k,\n",
        "                output_fields=[\"metadata\", \"documents\"]\n",
        "            )\n",
        "\n",
        "            print(f\"search results size : {len(search_results)}\")\n",
        "\n",
        "            for hits in search_results:\n",
        "                for hit in hits:\n",
        "                    print(f\"Collection: {collection_name}, data: {str(hit)}\")\n",
        "                    results.append({\n",
        "                        \"collection\": collection_name,\n",
        "                        \"id\": hit[\"id\"],\n",
        "                        \"metadata\": hit[\"entity\"][\"metadata\"],\n",
        "                        \"distance\": hit[\"distance\"],\n",
        "                        \"documents\": hit[\"entity\"][\"documents\"]\n",
        "                      })\n",
        "\n",
        "        results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        end_time = time.time()\n",
        "        print(f\"Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def hybrid_search(self, sparse_query_embedding: np.ndarray, dense_query_embedding: np.ndarray, collection='all', top_k : int=10, alpha=0.3)-> list[dict[str, any]] :\n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "        sparse_results = self.sparse_search(sparse_query_embedding, collection, top_k)\n",
        "        n = int(len(sparse_results) * alpha)\n",
        "        alpha_sparse_results = sparse_results[:n]\n",
        "        dense_results = self.search(dense_query_embedding, collection, top_k)\n",
        "        #'results = sorted(results, key=lambda x: x[\"distance\"])[:top_k]\n",
        "        results = dense_results + alpha_sparse_results\n",
        "        end_time = time.time()\n",
        "        print(f\"Hybrid Search completed. Found {len(results)} results. in {end_time - start_time} secs\")\n",
        "        return results\n",
        "\n",
        "    def extract_documents(self, search_results: list[dict[str, any]]) -> list[np.ndarray]:\n",
        "      \"\"\"\n",
        "      Extract embedding values from search results.\n",
        "      :param search_results: List of dictionaries containing search results.\n",
        "      :return: List of embedding vectors as NumPy arrays.\n",
        "      \"\"\"\n",
        "      return [result[\"documents\"] for result in search_results if \"documents\" in result]"
      ],
      "metadata": {
        "id": "5wJcp7qOieKd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['covidqa','cuad','delucionqa','emanual','expertqa','finqa','hagrid','hotpotqa','msmarco','pubmedqa','tatqa','techqa']\n",
        "datastor = VectorDataStore()\n",
        "\n",
        "insert_data = True\n",
        "store_client = \"Milvus\"\n",
        "num_records = 0\n",
        "\n",
        "vector_dim = embedder.get_sentence_embedding_dimension()"
      ],
      "metadata": {
        "id": "9Ug972iXDRwO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_data_index = random.randint(1, 12) - 1\n",
        "dataset = datasets[g_data_index]\n",
        "print(f\"Selected dataset: {datasets[g_data_index]}\")\n",
        "milvus_collection_name = f\"ragbench_collection_{dataset}_v16\"\n",
        "datastor.drop_collection(milvus_collection_name)\n",
        "\n",
        "data = load_dataset(\"rungalileo/ragbench\", datasets[g_data_index], split=\"train\")\n",
        "top_5_rows = data.select(range(5))"
      ],
      "metadata": {
        "id": "pwWHuni4h-Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Initialize storage for documents, IDs, and metadata\n",
        "\n",
        "for i in range(11):\n",
        "  all_documents = []\n",
        "  all_ids = []\n",
        "  all_metadatas = []\n",
        "\n",
        "  # Process each dataset\n",
        "  doc_idx = 0  # Global document index for unique IDs\n",
        "  for dataset in [datasets[i]]:\n",
        "      milvus_collection_name = f\"ragbench_collection_{dataset}_v16\"\n",
        "      try:\n",
        "        num_records = datastor.has_entities(milvus_collection_name)\n",
        "      except:\n",
        "        num_records = 0\n",
        "        print(\"we are good. collection doesnt exists\")\n",
        "\n",
        "      if num_records > 0:\n",
        "          datastor.drop_collection(milvus_collection_name)\n",
        "\n",
        "      datastor.get_or_create_collection(milvus_collection_name, vector_dim)\n",
        "\n",
        "      print(f\"Processing {dataset}...\")\n",
        "      data = load_dataset(\"rungalileo/ragbench\", dataset, split=\"train\")\n",
        "      #only select first 5 records for debugging duplicate records. **PLEASE REMOVE THIS AFTER DEBUGGING**\n",
        "      data = data.select(range(10))\n",
        "      for idx, row in tqdm(enumerate(data), desc=f\"Processing {dataset}\"):\n",
        "          # Extract document text\n",
        "          print(f\"~~~~~~>>>  question: {row.get('question', '')}\")\n",
        "          doc_text = row.get('documents', '')\n",
        "\n",
        "          # Skip if no documents found\n",
        "          if not doc_text:\n",
        "              continue\n",
        "\n",
        "          # Process the document\n",
        "          processed_output = process_document_with_identifiers(doc_text)\n",
        "          added_item_idxs = set()\n",
        "\n",
        "          # Populate the lists\n",
        "          for section_idx, section in enumerate(processed_output):\n",
        "              for item_idx, (prefix, content) in enumerate(section):\n",
        "                  # Skip if this item_idx has already been processed\n",
        "                  if item_idx in added_item_idxs:\n",
        "                      continue\n",
        "\n",
        "                  # Add the item_idx to the set to track it\n",
        "                  added_item_idxs.add(item_idx)\n",
        "\n",
        "                  # Add the document\n",
        "                  document = f\"[{prefix}] {content}\"\n",
        "                  all_documents.append(document)\n",
        "\n",
        "                  # Construct a globally unique ID\n",
        "                  doc_id = f\"{dataset}_{doc_idx}_{section_idx}_{item_idx}\"\n",
        "                  all_ids.append(doc_id)\n",
        "\n",
        "                  # Construct metadata\n",
        "                  metadata = {\n",
        "                      \"dataset\": dataset,\n",
        "                      \"global_index\": doc_idx,\n",
        "                      \"section_index\": section_idx,\n",
        "                      \"item_index\": item_idx,\n",
        "                      \"prefix\": prefix,\n",
        "                      \"type\": \"Title\" if prefix.endswith(\"a\") else \"Passage\",\n",
        "                  }\n",
        "                  all_metadatas.append(metadata)\n",
        "\n",
        "          doc_idx += 1  # Increment global document index\n",
        "\n",
        "  # Step 4: Generate Embeddings\n",
        "  embedder = SentenceTransformer(embedding_model)  # Pretrained sentence transformer\n",
        "  batch_size = 2500  # Adjust based on available memory\n",
        "\n",
        "  # Generate embeddings in batches\n",
        "  all_embeddings = []\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Generating embeddings\"):\n",
        "      batch_docs = all_documents[i:i + batch_size]\n",
        "      batch_embeddings = embedder.encode(batch_docs, show_progress_bar=True)\n",
        "      all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "  bm25_encoder = prepare_bm25_encoder(all_documents)\n",
        "  sparse_vectors = [generate_sparse_vector_bm25(text, bm25_encoder) for text in all_documents]\n",
        "\n",
        "  # Adding data to milvus with enhanced duplicate check\n",
        "  all_recs = datastor.get_all_records(milvus_collection_name)\n",
        "  #print(f\"sample: {str(all_recs[0])}\")\n",
        "  existing_hashes = get_existing_hashes_milvus(all_recs)\n",
        "\n",
        "  for i in tqdm(range(0, len(all_documents), batch_size), desc=\"Adding data to DB\"):\n",
        "      batch_embeddings = all_embeddings[i:i + batch_size]\n",
        "      batch_sparse_embs = sparse_vectors[i:i + batch_size]\n",
        "      batch_metadatas = all_metadatas[i:i + batch_size]\n",
        "      batch_documents = all_documents[i:i + batch_size]\n",
        "      batch_ids = []\n",
        "\n",
        "      # Generate hashes for each document in the batch\n",
        "      for doc, metadata in zip(batch_documents, batch_metadatas):\n",
        "          doc_hash = generate_hash(doc, metadata)\n",
        "          if doc_hash not in existing_hashes:\n",
        "              batch_ids.append(doc_hash)\n",
        "              existing_hashes.add(doc_hash)  # Add hash to local set to avoid duplicates in the same batch\n",
        "          else:\n",
        "              print(f\"Skipping duplicate document: {doc[:15]}...\")  # Print a preview of the duplicate doc\n",
        "\n",
        "      # Add non-duplicate documents to the database\n",
        "      if batch_ids:  # Ensure there are non-duplicate documents to add\n",
        "          # Add the batch to the Milvus collection\n",
        "          if store_client == \"Milvus\" and insert_data:\n",
        "              datastor.insert(milvus_collection_name,\n",
        "                  metadata=batch_metadatas,\n",
        "                  documents=batch_documents,\n",
        "                  sparse_embs = np.array(batch_sparse_embs),\n",
        "                  embeddings=np.array(batch_embeddings),\n",
        "                  ids=batch_ids\n",
        "              )\n",
        "\n",
        "  print(f\"total data inserted into {milvus_collection_name} iteration {i}\")\n",
        "  print(f\"records : {datastor.has_entities(milvus_collection_name)} \")"
      ],
      "metadata": {
        "id": "7V650exmiLNE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3385458a0ac4eb0be54260e74f97a7f",
            "63961e70a0ed449381a92e54d3aae1d8",
            "79507fb57fd1457b849e0e411e5b94e4",
            "9cacdba6588d4f1e8e37442fbf2d2bcd",
            "bc86b38342ae4738a4d878761f7eeebd",
            "c6ecfe8ab37e4986a24040408556c023",
            "f57a5e1c0ba14bb9a27bddcafffa4fc7",
            "b3af500badf746eda9dec18ce4209bc5",
            "ed42257f7c214553a724154dc6de74cf",
            "da21a2272efd4176b2f6b8a3ecb26112",
            "acd74cfad584461fbb5bc15ad78dbaee"
          ]
        },
        "outputId": "37f90cf4-1e05-426c-d55c-7d0ed70f3b6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are good. collection doesnt exists\n",
            "Collection ragbench_collection_covidqa_v16 doesn't exist. Creating...\n",
            "Processing covidqa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing covidqa: 2it [00:00, 16.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~>>>  question: What role does T-cell count play in severe human adenovirus type 55 (HAdV-55) infection?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Chen et al.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In our study, the only patient who recovered from severe infection had higher T-cell counts.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Three of the five patients had relatively low T-cell counts when admitted.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk\n",
            "section_chunks>>>>>>> [['0a', 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study\\nPassage: Recent studies have shown that the immune system plays a crucial role in the clearance of HAdV viremia and survival of the host .'], ['0a', 'Chen et al.'], ['0b', 'reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells .'], ['0c', 'In our study, the only patient who recovered from severe infection had higher T-cell counts.'], ['0d', 'Three of the five patients had relatively low T-cell counts when admitted.'], ['0e', 'Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Chen et al.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In our study, the only patient who recovered from severe infection had higher T-cell counts.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Three of the five patients had relatively low T-cell counts when admitted.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk\n",
            "section_chunks>>>>>>> [['1a', 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study\\nPassage: Recent studies have shown that the immune system plays a crucial role in the clearance of HAdV viremia and survival of the host .'], ['1a', 'Chen et al.'], ['1b', 'reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells .'], ['1c', 'In our study, the only patient who recovered from severe infection had higher T-cell counts.'], ['1d', 'Three of the five patients had relatively low T-cell counts when admitted.'], ['1e', 'Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Specifically, the HAdV-7-positive inpatients had lower white blood cell count , platelet count .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In contrast, hemoglobin and C-reactive protein levels, and the percentages of lymphocytes, neutrophils and positive sputum culture were found to be statistically similar.\n",
            "section_chunks>>>>>>> [['2a', 'Title: Human adenovirus type 7 infection causes a more severe disease than type 3\\nPassage: Laboratory findings for the HAdV-7-positive inpatients were also significantly different from those infected by HAdV-3 .'], ['2a', 'Specifically, the HAdV-7-positive inpatients had lower white blood cell count , platelet count .'], ['2b', 'In contrast, hemoglobin and C-reactive protein levels, and the percentages of lymphocytes, neutrophils and positive sputum culture were found to be statistically similar.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> White blood cell count > 15,000/ μL was defined as leukocytosis, whereas that < 4000/μL was defined as leukopenia.\n",
            "section_chunks>>>>>>> [['3a', 'Title: Human adenovirus type 7 infection causes a more severe disease than type 3\\nPassage: Imaging and laboratory data on admission and during hospitalization were collected.'], ['3a', 'White blood cell count > 15,000/ μL was defined as leukocytosis, whereas that < 4000/μL was defined as leukopenia.']]\n",
            "~~~~~~>>>  question: What is MVO?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The Morpholino moiety targets c-myc to inhibit proliferation of the vascular endothelial cells, which might otherwise cause restenosis.\n",
            "section_chunks>>>>>>> [['0a', 'Title: Gene Knockdowns in Adult Animals: PPMOs and Vivo-Morpholinos\\nPassage: Global Therapeutics, the cardiology unit of Cook Medical , is developing PPMO-coated stents for balloon angioplasty using PPMOs from AVI BioPharma Inc.'], ['0a', 'The Morpholino moiety targets c-myc to inhibit proliferation of the vascular endothelial cells, which might otherwise cause restenosis.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Recent studies explicitly comparing effects of unmodified Morpholino oligos and PPMOs show that the PPMOs are far more effective in vivo.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Viral applications of PPMOs have recently been reviewed so only a few recent papers applying PPMOs to inhibit viruses will be considered here.\n",
            "section_chunks>>>>>>> [['1a', 'Title: Gene Knockdowns in Adult Animals: PPMOs and Vivo-Morpholinos\\nPassage: Typically the highest antiviral efficacies are achieved with pre-infection administration of the PPMO followed by a series of post-infection doses.'], ['1a', 'Recent studies explicitly comparing effects of unmodified Morpholino oligos and PPMOs show that the PPMOs are far more effective in vivo.'], ['1b', 'Viral applications of PPMOs have recently been reviewed so only a few recent papers applying PPMOs to inhibit viruses will be considered here.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Compared to the GFP-encoding control virus MV vac2 -GFP, MVs encompassing the DisOva transgene cassette together with or without MLV gag grew with similar kinetics and reached similar maximum titers as the control virus.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Thus, cloning and rescue of both MVs-derived vectors resulted in expression of the inserted antigen in infected cells without impact on viral replication or genetic stability.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Antigen-specific cellular immunity against vector and transgene is induced by Ova-presenting MV.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> To\n",
            "section_chunks>>>>>>> [['2a', 'Title: Antigen-specific oncolytic MV-based tumor vaccines through presentation of selected tumor-associated antigens on infected cells or virus-like particles\\nPassage: To assess the replication efficiency of the different vectors, multi-step growth kinetics of cell-associated and released virus were performed following infection at low MOI = 0.03 .'], ['2a', 'Compared to the GFP-encoding control virus MV vac2 -GFP, MVs encompassing the DisOva transgene cassette together with or without MLV gag grew with similar kinetics and reached similar maximum titers as the control virus.'], ['2b', 'Thus, cloning and rescue of both MVs-derived vectors resulted in expression of the inserted antigen in infected cells without impact on viral replication or genetic stability.'], ['2c', 'Antigen-specific cellular immunity against vector and transgene is induced by Ova-presenting MV.'], ['2d', 'To']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> To was chosen since especially the cellular immune-responses and respective immune-dominant peptides in mice such as used in our studies are well known and can thus considerably support characterization of such responses.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> For this purpose, Ova was genetically fused to the transmembrane domain of the platelet-derived growth factor receptor giving rise to a membrane-bound extracellular variant that should be readily incorporated into retroviral particles .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> A recombinant MV encoding DisOva in an additional transcription unit following the P gene in combination with a second, MLV Gag-encoding transgene\n",
            "section_chunks>>>>>>> [['3a', 'Title: Antigen-specific oncolytic MV-based tumor vaccines through presentation of selected tumor-associated antigens on infected cells or virus-like particles\\nPassage: Generation and characterization of recombinant MV vac2 encoding different forms of the Ovaantigen.'], ['3a', 'To was chosen since especially the cellular immune-responses and respective immune-dominant peptides in mice such as used in our studies are well known and can thus considerably support characterization of such responses.'], ['3b', 'For this purpose, Ova was genetically fused to the transmembrane domain of the platelet-derived growth factor receptor giving rise to a membrane-bound extracellular variant that should be readily incorporated into retroviral particles .'], ['3c', 'A recombinant MV encoding DisOva in an additional transcription unit following the P gene in combination with a second, MLV Gag-encoding transgene']]\n",
            "~~~~~~>>>  question: What have sero-surveys of MERS virus found?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> These relied on MERS-CoV-infected cell culture as an antigen source, detecting the presence of human anti-MERS-CoV IgG, IgM or neutralizing antibodies in human samples .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> No sign of MERS-CoV antibodies was found among 2,400 sera from patients visiting Hospital in Jeddah, from 2010 through 2012, prior to the description of MERS-CoV .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Nor did IFA methods detect any sign of prior MERS-CoV infection among a small sample of 130 healthy blood donors from another Hospital in Jeddah .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Of 226 slaughterhouse workers, only eight were positive by IFA, and those sera could not be confirmed by virus\n",
            "section_chunks>>>>>>> [['0a', 'Title: MERS coronavirus: diagnostics, epidemiology and transmission\\nPassage: for human sero-surveys.'], ['0a', 'These relied on MERS-CoV-infected cell culture as an antigen source, detecting the presence of human anti-MERS-CoV IgG, IgM or neutralizing antibodies in human samples .'], ['0b', 'No sign of MERS-CoV antibodies was found among 2,400 sera from patients visiting Hospital in Jeddah, from 2010 through 2012, prior to the description of MERS-CoV .'], ['0c', 'Nor did IFA methods detect any sign of prior MERS-CoV infection among a small sample of 130 healthy blood donors from another Hospital in Jeddah .'], ['0d', 'Of 226 slaughterhouse workers, only eight were positive by IFA, and those sera could not be confirmed by virus']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Much of the Arabian Peninsula and all of the Horn of Africa lack baseline data describing the proportion of the community who may have been infected by a MERS-CoV.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> However, sero-surveys have had widespread use in elucidating the role of DCs as a transmission source for MERS-CoV.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Because of the identity shared between DC and human MERS-CoV , serological assays for DC sero-surveys should be transferrable to human screening with minimal re-configuration.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Also, no diagnostically relevant variation in neutralization activity have been found from among a range of\n",
            "section_chunks>>>>>>> [['1a', 'Title: MERS coronavirus: diagnostics, epidemiology and transmission\\nPassage: Strategic, widespread sero-surveys of humans using samples collected after 2012 are infrequent.'], ['1a', 'Much of the Arabian Peninsula and all of the Horn of Africa lack baseline data describing the proportion of the community who may have been infected by a MERS-CoV.'], ['1b', 'However, sero-surveys have had widespread use in elucidating the role of DCs as a transmission source for MERS-CoV.'], ['1c', 'Because of the identity shared between DC and human MERS-CoV , serological assays for DC sero-surveys should be transferrable to human screening with minimal re-configuration.'], ['1d', 'Also, no diagnostically relevant variation in neutralization activity have been found from among a range of']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> A stage 2 seropositive result additionally required a suitably titred PRNT result .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The study found 15 sera collected in 2012 to 2013 from 10,009 people in 13 KSA provinces contained MERS-CoV antibodies, but significantly higher proportions in occurred in camel shepherds and slaughterhouse workers .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Contemporary surveys are needed.\n",
            "section_chunks>>>>>>> [['2a', \"Title: MERS coronavirus: diagnostics, epidemiology and transmission\\nPassage: human sera, a tiered diagnostic process assigned both recombinant IFA and recombinant ELISA positive sera to 'stage 1' seropositivity.\"], ['2a', 'A stage 2 seropositive result additionally required a suitably titred PRNT result .'], ['2b', 'The study found 15 sera collected in 2012 to 2013 from 10,009 people in 13 KSA provinces contained MERS-CoV antibodies, but significantly higher proportions in occurred in camel shepherds and slaughterhouse workers .'], ['2c', 'Contemporary surveys are needed.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> DCs were sampled from a mostly Canary Island-born herd and from Omani DCs .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> A neutralising antibody assay found only 10 % of strongly seropositive Canary Island .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> b Camel-to-human infections appear to be infrequent, while human-to-human spread of infection is regularly facilitated by poor IPC in healthcare settings where transmission is amplified, accounting for the bulk of cases.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> There are human MERS cases that do not fall into either category of source and it is unclear if these acquired infection through some entirely\n",
            "section_chunks>>>>>>> [['3a', 'Title: MERS coronavirus: diagnostics, epidemiology and transmission\\nPassage: The first sero-survey of livestock living in the Middle East region was conducted during 2012-2013 .'], ['3a', 'DCs were sampled from a mostly Canary Island-born herd and from Omani DCs .'], ['3b', 'A neutralising antibody assay found only 10 % of strongly seropositive Canary Island .'], ['3c', 'b Camel-to-human infections appear to be infrequent, while human-to-human spread of infection is regularly facilitated by poor IPC in healthcare settings where transmission is amplified, accounting for the bulk of cases.'], ['3d', 'There are human MERS cases that do not fall into either category of source and it is unclear if these acquired infection through some entirely']]\n",
            "~~~~~~>>>  question: How were untreated MDA-MB-231 cells labeled?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The cells were cultured for 3 days until tumorsphere formation.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The tumorsphere were then treated with anticancer drugs for 48 h and stained with Calcein AM and Ethidium homodimer-1 for live and dead cell population, respectively.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The cells treated with or without 70% methanol for 30 min were considered as a relative control for all dead or live cells.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The live and dead cells were then observed under microscopy and quantitated with Fluoroskan Ascent FL reader as\n",
            "section_chunks>>>>>>> [['0a', 'Title: Kinome-Wide siRNA Screening Identifies Src-Enhanced Resistance of Chemotherapeutic Drugs in Triple-Negative Breast Cancer Cells\\nPassage: MDA-MB-231 and Hs578T cells were seeded at density of 2 × 10 4 in NanoCulture plates in the presence of 10 nM siRNA.'], ['0a', 'The cells were cultured for 3 days until tumorsphere formation.'], ['0b', 'The tumorsphere were then treated with anticancer drugs for 48 h and stained with Calcein AM and Ethidium homodimer-1 for live and dead cell population, respectively.'], ['0c', 'The cells treated with or without 70% methanol for 30 min were considered as a relative control for all dead or live cells.'], ['0d', 'The live and dead cells were then observed under microscopy and quantitated with Fluoroskan Ascent FL reader as']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Briefly, PBMCs were isolated from human whole blood by density-gradient centrifugation over Histopaque .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Monocytes were purified using human CD14-specific microbeads following manufacturer's instructions.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> CD14 + monocytes were differentiated into MDMs by culturing for 6-7 days with recombinant human macrophage colony-stimulating factor and conditioned medium from KPB-M15 cells .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Media were replaced every 2-3 days during the incubation for a total of 6-7 days.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The cells were harvested and plated on desired 96-well plates 1 day prior to the drug screen assay.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The differentiated MDMs were characterized by flow\n",
            "section_chunks>>>>>>> [['1a', 'Title: Testing therapeutics in cell-based assays: Factors that influence the apparent potency of drugs\\nPassage: The generation of MDMs has been described in previous studies .'], ['1a', 'Briefly, PBMCs were isolated from human whole blood by density-gradient centrifugation over Histopaque .'], ['1b', \"Monocytes were purified using human CD14-specific microbeads following manufacturer's instructions.\"], ['1c', 'CD14 + monocytes were differentiated into MDMs by culturing for 6-7 days with recombinant human macrophage colony-stimulating factor and conditioned medium from KPB-M15 cells .'], ['1d', 'Media were replaced every 2-3 days during the incubation for a total of 6-7 days.'], ['1e', 'The cells were harvested and plated on desired 96-well plates 1 day prior to the drug screen assay.'], ['1f', 'The differentiated MDMs were characterized by flow']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Cells were next incubated for 30 min at room temperature with a 1:4 dilution of day 14 sera from PSMA-DMAb plasmid-injected mice and then washed.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Finally, cells were incubated in the dark for 30 min with a 1:100 dilution of PE-conjugated anti-human Fc IgG , followed by a final wash with FACS buffer.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Samples were resuspended in 1× stabilizing fixative\n",
            "section_chunks>>>>>>> [['2a', 'Title: Novel prostate cancer immunotherapy with a DNA-encoded anti-prostate-specific membrane antigen monoclonal antibody\\nPassage: To detect cell surface PSMA, tubes of 1.0 × 10 6 LNCaP or TRAMP-C2 cells were washed with phosphate-buffered saline , stained with live/dead fixable violet dead cell stain for 15 min, and then washed twice with FACS buffer .'], ['2a', 'Cells were next incubated for 30 min at room temperature with a 1:4 dilution of day 14 sera from PSMA-DMAb plasmid-injected mice and then washed.'], ['2b', 'Finally, cells were incubated in the dark for 30 min with a 1:100 dilution of PE-conjugated anti-human Fc IgG , followed by a final wash with FACS buffer.'], ['2c', 'Samples were resuspended in 1× stabilizing fixative']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> DBTRG .05MG cells were seeded in 6-well plate and incubated at 37 o C in 5%CO 2 atmosphere.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Twenty-four hours later, the medium in each well was removed and replaced with NDV at IC 50 concentration dissolved in the culture medium and incubated at 37 o C in 5%CO 2 atmosphere for 72 hours.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> After the incubation period, detached cells in the medium were collected and added back to trypsinised adherent cells.\n",
            "section_chunks>>>>>>> [['3a', 'Title: Evaluation of Ultra-Microscopic Changes and Proliferation of Apoptotic Glioblastoma Multiforme Cells Induced by Velogenic Strain of Newcastle Disease Virus AF2240\\nPassage: Acridine Orange Double staining DBTRG .05MG cells were stained using propidium iodide and acridine-orange double staining according to standard procedures and examine under a fluorescence microscope N .'], ['3a', 'DBTRG .05MG cells were seeded in 6-well plate and incubated at 37 o C in 5%CO 2 atmosphere.'], ['3b', 'Twenty-four hours later, the medium in each well was removed and replaced with NDV at IC 50 concentration dissolved in the culture medium and incubated at 37 o C in 5%CO 2 atmosphere for 72 hours.'], ['3c', 'After the incubation period, detached cells in the medium were collected and added back to trypsinised adherent cells.']]\n",
            "~~~~~~>>>  question: What is the number of inhabitants of Reunion Island?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Cumulative incidence rates of infection and seroconversion rates were standardized according to the age structure of the community source).\n",
            "section_chunks>>>>>>> [['0a', 'Title: Pandemic Influenza Due to pH1N1/2009 Virus: Estimation of Infection Burden in Reunion Island through a Prospective Serosurvey, Austral Winter 2009\\nPassage: the population of Reunion Island and a Chi2 test was used to analyse differences in age, sex and geographic location.'], ['0a', 'Cumulative incidence rates of infection and seroconversion rates were standardized according to the age structure of the community source).']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Taking into account those who did not consult a physician, the number of symptomatic infected persons was estimated to 104,067 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In fact, the attack rate of pH1N1/2009 infection in our serosurvey was about 42%-44% at the peak of the antibody response , a figure which is at least 3 to 4 times higher than rates\n",
            "section_chunks>>>>>>> [['1a', 'Title: Pandemic Influenza Due to pH1N1/2009 Virus: Estimation of Infection Burden in Reunion Island through a Prospective Serosurvey, Austral Winter 2009\\nPassage: Based on clinical cases reported to the epidemiological surveillance services , it was estimated that 66,915 persons in Reunion Island who consulted a physician were infected by the pH1N1/2009 virus during the 9 weeks of the epidemic, giving a cumulative attack rate of 8.26%.'], ['1a', 'Taking into account those who did not consult a physician, the number of symptomatic infected persons was estimated to 104,067 .'], ['1b', 'In fact, the attack rate of pH1N1/2009 infection in our serosurvey was about 42%-44% at the peak of the antibody response , a figure which is at least 3 to 4 times higher than rates']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The first imported case of pH1N1/2009v was identified on 5 th July 2009 in a traveller returning from Australia.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The first case indicating community transmission was detected on 21 st July .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> pH1N1/2009v became the predominant circulating influenza virus within four weeks of its first detection, its activity peaked during week 35 and ended at week 38 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Contrary to initial fears, the health care system was not overwhelmed, as morbidity and mortality rates were lower\n",
            "section_chunks>>>>>>> [['2a', 'Title: Pandemic Influenza Due to pH1N1/2009 Virus: Estimation of Infection Burden in Reunion Island through a Prospective Serosurvey, Austral Winter 2009\\nPassage: Reunion Island is a French overseas department located in the southwestern Indian Ocean, 700 km east of Madagascar and 200 km southwest of Mauritius.'], ['2a', 'The first imported case of pH1N1/2009v was identified on 5 th July 2009 in a traveller returning from Australia.'], ['2b', 'The first case indicating community transmission was detected on 21 st July .'], ['2c', 'pH1N1/2009v became the predominant circulating influenza virus within four weeks of its first detection, its activity peaked during week 35 and ended at week 38 .'], ['2d', 'Contrary to initial fears, the health care system was not overwhelmed, as morbidity and mortality rates were lower']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The island benefits from a healthcare system similar to mainland France and epidemiological surveillance has been developed by the regional office of the French Institute for Public Health Surveillance , based on the surveillance system of mainland France .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Influenza activity generally increases during austral winter, corresponding to summer in Europe ."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing covidqa: 10it [00:00, 31.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Since 2011, influenza vaccination campaign in Reunion Island starts in April and the vaccine used corresponds to World Health Organization recommendations for the southern\n",
            "section_chunks>>>>>>> [['3a', 'Title: Etiology of Influenza-Like Illnesses from Sentinel Network Practitioners in Réunion Island, 2011-2012\\nPassage: Réunion Island, a French overseas territory with 850,000 inhabitants, is located in the southern hemisphere between Madagascar and Mauritius in the Indian Ocean .'], ['3a', 'The island benefits from a healthcare system similar to mainland France and epidemiological surveillance has been developed by the regional office of the French Institute for Public Health Surveillance , based on the surveillance system of mainland France .'], ['3b', 'Influenza activity generally increases during austral winter, corresponding to summer in Europe .'], ['3c', 'Since 2011, influenza vaccination campaign in Reunion Island starts in April and the vaccine used corresponds to World Health Organization recommendations for the southern']]\n",
            "~~~~~~>>>  question: What genome sequence was available for this study?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Illumina read data was submitted to NCBI's Short Read Archive under Trace Identifiers SRR513075, SRR513078, SRR513080, SRR513086-87, SRR513092, and SRR527699-726.\n",
            "section_chunks>>>>>>> [['0a', \"Title: Complete viral RNA genome sequencing of ultra-low copy samples by sequence-independent amplification\\nPassage: All consensus genome assemblies generated as part of this project were submitted to NCBI's GenBank database .\"], ['0a', \"Illumina read data was submitted to NCBI's Short Read Archive under Trace Identifiers SRR513075, SRR513078, SRR513080, SRR513086-87, SRR513092, and SRR527699-726.\"]]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The two synthetic variants of the virus were synthesized using two different methods .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The WT sequence can be found in Additional file 1, section 8.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Sequencing libraries were prepared using the INCPM DNA-seq protocol, and sequenced 2 × 150 on an Illumina MiSeq nano v2 PE150.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Sequenced reads were mapped to a reference genome using BWA MEM v0.75 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Among all read-pairs with the same alignment, a single representative read was\n",
            "section_chunks>>>>>>> [['1a', 'Title: Generation and comparative genomics of synthetic dengue viruses\\nPassage: Three DENV cDNA samples were sequenced and analyzed: one of the samples was the wildtype strain, with the WT sequence used as a reference genome for NGS analysis.'], ['1a', 'The two synthetic variants of the virus were synthesized using two different methods .'], ['1b', 'The WT sequence can be found in Additional file 1, section 8.'], ['1c', 'Sequencing libraries were prepared using the INCPM DNA-seq protocol, and sequenced 2 × 150 on an Illumina MiSeq nano v2 PE150.'], ['1d', 'Sequenced reads were mapped to a reference genome using BWA MEM v0.75 .'], ['1e', 'Among all read-pairs with the same alignment, a single representative read was']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Primer sequences are available upon request.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The sequencing reaction was performed by ABI PRISM ® BigDye™ Terminators v3.1 Cycle Sequencing Kit as described previously .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The sequences were developed on an automatic ABI PRISM ® 3130 genetic analyzer with 80 cm capillaries.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Consensus sequences were generated in SeqScape ® Software v2.5 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Sequence assembly, multiple alignment and alignment trimming were performed with the BioEdit software v.7.0.5 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Distance based neighbor joining phylogenetic trees and character based maximum parsimony trees were generated using the Molecular Evolutionary Genetics Analysis software v.3.1 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Maximum likelihood trees were\n",
            "section_chunks>>>>>>> [['2a', 'Title: The evolution of human influenza A viruses from 1999 to 2006: A complete genome study\\nPassage: Purified PCR products were sequenced directly.'], ['2a', 'Primer sequences are available upon request.'], ['2b', 'The sequencing reaction was performed by ABI PRISM ® BigDye™ Terminators v3.1 Cycle Sequencing Kit as described previously .'], ['2c', 'The sequences were developed on an automatic ABI PRISM ® 3130 genetic analyzer with 80 cm capillaries.'], ['2d', 'Consensus sequences were generated in SeqScape ® Software v2.5 .'], ['2e', 'Sequence assembly, multiple alignment and alignment trimming were performed with the BioEdit software v.7.0.5 .'], ['2f', 'Distance based neighbor joining phylogenetic trees and character based maximum parsimony trees were generated using the Molecular Evolutionary Genetics Analysis software v.3.1 .'], ['2g', 'Maximum likelihood trees were']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The 686 sequencing libraries thus originated from 252 different cancer samples, 32 non-template controls, and 24 exogenous controls.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The distribution of methods, libraries and controls for each sample type is provided in Table S2 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Samples were preferably analysed with multiple methods, thus 165 out of 252 samples were analysed with more than one laboratory method .\n",
            "section_chunks>>>>>>> [['3a', 'Title: Identification of Known and Novel Recurrent Viral Sequences in Data from Multiple Patients and Multiple Cancers\\nPassage: Ultimately, the data set consisted of 686 DNA and RNA libraries, for which 2ˆ100 bp paired end sequencing was performed using the Illumina HiSeq 2000 platform at BGI-Europe, Copenhagen, Denmark.'], ['3a', 'The 686 sequencing libraries thus originated from 252 different cancer samples, 32 non-template controls, and 24 exogenous controls.'], ['3b', 'The distribution of methods, libraries and controls for each sample type is provided in Table S2 .'], ['3c', 'Samples were preferably analysed with multiple methods, thus 165 out of 252 samples were analysed with more than one laboratory method .']]\n",
            "~~~~~~>>>  question: What is  the destabilization is further compounded by?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> As spatial effects are further studied in epidemic models, it remains to be seen how this phenomenon will extend to more complicated spatial geometries, including more patches and perhaps non-symmetric coupling terms.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This work represents a first step towards understanding the role of migration and spatial heterogeneity in dynamical properties of dengue observed in epidemiological data, such as traveling waves of infection in Thailand .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Furthermore, because the migration-induced stabilization depends only on the existence of a Hopf bifurcation in the model, it is expected\n",
            "section_chunks>>>>>>> [['0a', 'Title: Asymmetry in the Presence of Migration Stabilizes Multistrain Disease Outbreaks\\nPassage: Finally, the work discussed here shows a potential effect of human movement between heterogeneous regions.'], ['0a', 'As spatial effects are further studied in epidemic models, it remains to be seen how this phenomenon will extend to more complicated spatial geometries, including more patches and perhaps non-symmetric coupling terms.'], ['0b', 'This work represents a first step towards understanding the role of migration and spatial heterogeneity in dynamical properties of dengue observed in epidemiological data, such as traveling waves of infection in Thailand .'], ['0c', 'Furthermore, because the migration-induced stabilization depends only on the existence of a Hopf bifurcation in the model, it is expected']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Our results suggest that if control strategies in one region are able to generate enough asymmetry, this could lead to a stabilization of the outbreaks, which would have a positive effect on adjacent regions.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Asymmetry could be generated in the effective contact rate by mosquito control, which could include reducing mosquito breeding sites , or through new genetic controls which are under development .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Asymmetry in the birth rate could be generated\n",
            "section_chunks>>>>>>> [['1a', 'Title: Asymmetry in the Presence of Migration Stabilizes Multistrain Disease Outbreaks\\nPassage: Bifurcations from steady state to oscillatory behavior can be associated with an increased number of infection cases, particularly if chaotic oscillations occur, as in previous dengue models .'], ['1a', 'Our results suggest that if control strategies in one region are able to generate enough asymmetry, this could lead to a stabilization of the outbreaks, which would have a positive effect on adjacent regions.'], ['1b', 'Asymmetry could be generated in the effective contact rate by mosquito control, which could include reducing mosquito breeding sites , or through new genetic controls which are under development .'], ['1c', 'Asymmetry in the birth rate could be generated']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> We again saw that coupling between asymmetric systems led to stabilization.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This indicates that the stabilization in the epidemic model is a result of the underlying dynamics, rather than the details of the model.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> We suggest that the stabilization may occur as a result of the two different coupled frequencies generating oscillations that tend to cancel each other because of phase differences.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This topic will be studied in more detail in a future work.\n",
            "section_chunks>>>>>>> [['2a', 'Title: Asymmetry in the Presence of Migration Stabilizes Multistrain Disease Outbreaks\\nPassage: To motivate this result, we diffusively coupled two low dimensional Hopf bifurcations with different characteristic frequencies and analyzed the stability of the steady state.'], ['2a', 'We again saw that coupling between asymmetric systems led to stabilization.'], ['2b', 'This indicates that the stabilization in the epidemic model is a result of the underlying dynamics, rather than the details of the model.'], ['2c', 'We suggest that the stabilization may occur as a result of the two different coupled frequencies generating oscillations that tend to cancel each other because of phase differences.'], ['2d', 'This topic will be studied in more detail in a future work.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Structural inequalities coupled with civil unrest already produce widespread hunger, malnourishment and famine.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> An environmental disaster in a large population, most obviously China or India, could have devastating effects within the world system.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This, of course, is the great fear environmentalists have for dependence on mono-cropping, species loss, and water management 5 .\n",
            "section_chunks>>>>>>> [['3a', 'Title: Globalization and emerging governance modalities\\nPassage: cannot afford grain.'], ['3a', 'Structural inequalities coupled with civil unrest already produce widespread hunger, malnourishment and famine.'], ['3b', 'An environmental disaster in a large population, most obviously China or India, could have devastating effects within the world system.'], ['3c', 'This, of course, is the great fear environmentalists have for dependence on mono-cropping, species loss, and water management 5 .']]\n",
            "~~~~~~>>>  question: How many amino acids are in the SARS-CoV E protein?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Passage: The S protein of CoVs is inserted in the envelope of the virion mediating binding and fusion events necessary for infection, and it is the major target of the humoral protective immunity .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Although the S protein of SARS-CoV shares little aminoacid identity , it shares common structural features with S proteins of the other members of the Coronaviridae family.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> SARS-S protein is a type I transmembrane glycoprotein of approximately 1,255 amino acids in length and divided into two functional domains: S1 and S2 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In many CoVs, the S protein is cleaved during biogenesis and these two functional domains\n",
            "section_chunks>>>>>>> [['0a', 'Title: Neutralization Interfering Antibodies: A “Novel” Example of Humoral Immune Dysfunction Facilitating Viral Escape?'], ['0a', 'Passage: The S protein of CoVs is inserted in the envelope of the virion mediating binding and fusion events necessary for infection, and it is the major target of the humoral protective immunity .'], ['0b', 'Although the S protein of SARS-CoV shares little aminoacid identity , it shares common structural features with S proteins of the other members of the Coronaviridae family.'], ['0c', 'SARS-S protein is a type I transmembrane glycoprotein of approximately 1,255 amino acids in length and divided into two functional domains: S1 and S2 .'], ['0d', 'In many CoVs, the S protein is cleaved during biogenesis and these two functional domains']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> In addition, the SARS-CoV 3a protein contains a cysteine-rich domain that is involved in the formation of a homodimer to generate the ion channel .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Thus, mutation of the cysteine-rich domain blocks the ion conductivity by the 3a protein .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> To this end, we substituted amino acids Cys-127, Cys-130, and Cys-133 within the cysteine-rich domain of the SARS-CoV 3a protein with serine to generate a lentivirus expressing the ion\n",
            "section_chunks>>>>>>> [['1a', 'Title: Severe Acute Respiratory Syndrome Coronavirus Viroporin 3a Activates the NLRP3 Inflammasome\\nPassage: Previous studies demonstrated that the N-terminal 40 amino acids of the SARS-CoV E protein are important for ion channel formation, and that mutations N15A and V25F ] prevent ion conductivity .'], ['1a', 'In addition, the SARS-CoV 3a protein contains a cysteine-rich domain that is involved in the formation of a homodimer to generate the ion channel .'], ['1b', 'Thus, mutation of the cysteine-rich domain blocks the ion conductivity by the 3a protein .'], ['1c', 'To this end, we substituted amino acids Cys-127, Cys-130, and Cys-133 within the cysteine-rich domain of the SARS-CoV 3a protein with serine to generate a lentivirus expressing the ion']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The S1 subunit contains a signal peptide, followed by an N-terminal domain and receptor-binding domain , while the S2 subunit contains conserved fusion peptide , heptad repeat 1 and 2, transmembrane domain , and cytoplasmic domain .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> We found that the S2 subunit of 2019-nCoV is highly conserved and shares 99% identity with those of the two bat SARS-like CoVs and human SARS-CoV .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Thus the broad spectrum antiviral peptides against S2 would be an important preventive and treatment modality for testing in animal models before clinical trials .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Though the S1\n",
            "section_chunks>>>>>>> [['2a', 'Title: Genomic characterization of the 2019 novel human-pathogenic coronavirus isolated from a patient with atypical pneumonia after visiting Wuhan\\nPassage: Spike glycoprotein comprised of S1 and S2 subunits.'], ['2a', 'The S1 subunit contains a signal peptide, followed by an N-terminal domain and receptor-binding domain , while the S2 subunit contains conserved fusion peptide , heptad repeat 1 and 2, transmembrane domain , and cytoplasmic domain .'], ['2b', 'We found that the S2 subunit of 2019-nCoV is highly conserved and shares 99% identity with those of the two bat SARS-like CoVs and human SARS-CoV .'], ['2c', 'Thus the broad spectrum antiviral peptides against S2 would be an important preventive and treatment modality for testing in animal models before clinical trials .'], ['2d', 'Though the S1']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> These correspond to DENV-2 strain S1 E protein amino acids 41-60, 131-150, 251-270, and 351-370 that were selected for synthesis and antiviral testing .\n",
            "section_chunks>>>>>>> [['3a', 'Title: Structural Optimization and De Novo Design of Dengue Virus Entry Inhibitory Peptides\\nPassage: with the potential for the highest in situ binding affinities.'], ['3a', 'These correspond to DENV-2 strain S1 E protein amino acids 41-60, 131-150, 251-270, and 351-370 that were selected for synthesis and antiviral testing .']]\n",
            "~~~~~~>>>  question: What is the  the rate of general transmission, even in outbreaks?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> About half had an initial R of 1 or less, and over two-thirds had an initial R of 3 or less.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The median duration, as measured by number of generations, was 1 with a range of 0 to 9 .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> About a third did not extend beyond the index generation, and nearly three quarters lasted for 3 or fewer generations.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The median outbreak size, as measured by the total number of cases, was 4 with a range of 1 to 134\n",
            "section_chunks>>>>>>> [['0a', 'Title: Transmission patterns of smallpox: systematic review of natural outbreaks in Europe and North America since World War II\\nPassage: The median initial reproduction rate across all 51 outbreaks was 2 with a range of 0 to 38 .'], ['0a', 'About half had an initial R of 1 or less, and over two-thirds had an initial R of 3 or less.'], ['0b', 'The median duration, as measured by number of generations, was 1 with a range of 0 to 9 .'], ['0c', 'About a third did not extend beyond the index generation, and nearly three quarters lasted for 3 or fewer generations.'], ['0d', 'The median outbreak size, as measured by the total number of cases, was 4 with a range of 1 to 134']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> About a third had an initial R of 1 or less, and about two thirds had an initial R of 3 or less.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Outbreaks that remained in the community had a lower initial R than those that were hospital based from the first generation .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Mixed outbreaks, which generally started in the community and later spread through hospital contacts, had an intermediate initial R .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The initial R was smaller when the index case had a typical versus atypical or hemorrhagic\n",
            "section_chunks>>>>>>> [['1a', 'Title: Transmission patterns of smallpox: systematic review of natural outbreaks in Europe and North America since World War II\\nPassage: The median initial R across the 30 detailed outbreaks was 2 with a range of 0 to 38 .'], ['1a', 'About a third had an initial R of 1 or less, and about two thirds had an initial R of 3 or less.'], ['1b', 'Outbreaks that remained in the community had a lower initial R than those that were hospital based from the first generation .'], ['1c', 'Mixed outbreaks, which generally started in the community and later spread through hospital contacts, had an intermediate initial R .'], ['1d', 'The initial R was smaller when the index case had a typical versus atypical or hemorrhagic']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> For example, in outbreaks with an initial R of 5 or less versus more than 5, median values for the total number of cases and generations were 2 versus 23 and 1 versus 2.5 respectively.\n",
            "section_chunks>>>>>>> [['2a', 'Title: Transmission patterns of smallpox: systematic review of natural outbreaks in Europe and North America since World War II\\nPassage: Outbreaks with greater initial R-values tended to be larger and longer.'], ['2a', 'For example, in outbreaks with an initial R of 5 or less versus more than 5, median values for the total number of cases and generations were 2 versus 23 and 1 versus 2.5 respectively.']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This outbreak was the largest identified for this review .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Moreover, the hospital and mixed component of this outbreak had the second highest and highest initial reproductive rate, and the hospital component lasted 9 generations .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> This suggests that a smallpox outbreak in a less developed country with limited resources for healthcare, disease surveillance, and case isolation could be potentially more devastating than a bioterrorist attack in a Western/industrialized country .\n",
            "section_chunks>>>>>>> [['3a', 'Title: Transmission patterns of smallpox: systematic review of natural outbreaks in Europe and North America since World War II\\nPassage: Finally, the outbreak from Kosovo deserves notice because this region is not as developed or as wealthy as most of the other countries studied.'], ['3a', 'This outbreak was the largest identified for this review .'], ['3b', 'Moreover, the hospital and mixed component of this outbreak had the second highest and highest initial reproductive rate, and the hospital component lasted 9 generations .'], ['3c', 'This suggests that a smallpox outbreak in a less developed country with limited resources for healthcare, disease surveillance, and case isolation could be potentially more devastating than a bioterrorist attack in a Western/industrialized country .']]\n",
            "~~~~~~>>>  question: What primer pairs were used for PCR?\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> All primers used in this study had a thymine nucleotide at the 59 end to minimize addition of non-templated adenosines during amplification using Taq polymerase .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The sensitivity of each RT-PCR primer pair was determined using known quantities of a synthetic calibrant RNA template as described previously .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Each of the primer pairs was sensitive to as few as twenty copies of the calibrant RNA and several primers were sensitive to five copies .\n",
            "section_chunks>>>>>>> [['0a', 'Title: Global Surveillance of Emerging Influenza Virus Genotypes by Mass Spectrometry\\nPassage: primer pairs targeting NP and PB2 segments.'], ['0a', 'All primers used in this study had a thymine nucleotide at the 59 end to minimize addition of non-templated adenosines during amplification using Taq polymerase .'], ['0b', 'The sensitivity of each RT-PCR primer pair was determined using known quantities of a synthetic calibrant RNA template as described previously .'], ['0c', 'Each of the primer pairs was sensitive to as few as twenty copies of the calibrant RNA and several primers were sensitive to five copies .']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> These correspond to copy number ratios of vaccinia:human genomes of 1.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> All PCR experiments were analyzed on 3% agarose TBE gels containing ethidum bromide that were purchased from Bio-Rad .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Blue juice TM 10Â loading dye was purchased from Invitrogen and diluted to 2Â before use.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> A 50-bp DNA ladder was purchased from Novagen .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> For analysis, 15 ml from each separate 25 ml PCR reaction were combined with 2 ml of of loading dye and 15 ml of the loading-dye/product mixture was loaded per well and electrophoresed\n",
            "section_chunks>>>>>>> [['1a', 'Title: Multiplex primer prediction software for divergent targets\\nPassage: : 1, 100 : 1, 1000 : 1 and 10000 : 1.'], ['1a', 'These correspond to copy number ratios of vaccinia:human genomes of 1.'], ['1b', 'All PCR experiments were analyzed on 3% agarose TBE gels containing ethidum bromide that were purchased from Bio-Rad .'], ['1c', 'Blue juice TM 10Â loading dye was purchased from Invitrogen and diluted to 2Â before use.'], ['1d', 'A 50-bp DNA ladder was purchased from Novagen .'], ['1e', 'For analysis, 15 ml from each separate 25 ml PCR reaction were combined with 2 ml of of loading dye and 15 ml of the loading-dye/product mixture was loaded per well and electrophoresed']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Nanda et al.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> were able to achieve sufficient viral isolation from cell culture samples to allow viral identification using viral PCR with priming sequences as short as pentamers, so the problem of contaminating host nucleic acids for specific, short primer PCR of viruses is not insurmountable.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> They found specific pentamer PCR to be several logs more sensitive than nonspecific amplification, provided that they purified encapsidated viral nucleic acids prior to PCR.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Another method that has been used for\n",
            "section_chunks>>>>>>> [['2a', 'Title: Multiplex primer prediction software for divergent targets\\nPassage: acids could be problematic for universal viral priming using primers shorter than 15 bases, particularly for multiplexes of 10 or more primers.'], ['2a', 'Nanda et al.'], ['2b', 'were able to achieve sufficient viral isolation from cell culture samples to allow viral identification using viral PCR with priming sequences as short as pentamers, so the problem of contaminating host nucleic acids for specific, short primer PCR of viruses is not insurmountable.'], ['2c', 'They found specific pentamer PCR to be several logs more sensitive than nonspecific amplification, provided that they purified encapsidated viral nucleic acids prior to PCR.'], ['2d', 'Another method that has been used for']]\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Primers were designed using the Primer Premier 5.0 program.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> Like a TaqMan probe, the HFman probe was designed in general with a fluorophore and a quencher at 5′ and 3′ end, respectively; however, a better form of the HFman probe is to have a 3′ fluorophore and a 5′ quencher.\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> All primers were synthesized commercially by Thermofisher scientific .\n",
            "chunked_texts>>>>process_document_with_identifiers>>>>> The detailed information of the primers, TaqMan and HFman probes are shown in Supplementary Tables S1 and S2.\n",
            "section_chunks>>>>>>> [['3a', 'Title: A novel quantitative PCR mediated by high-fidelity DNA polymerase\\nPassage: Oligonucleotides design.'], ['3a', 'Primers were designed using the Primer Premier 5.0 program.'], ['3b', 'Like a TaqMan probe, the HFman probe was designed in general with a fluorophore and a quencher at 5′ and 3′ end, respectively; however, a better form of the HFman probe is to have a 3′ fluorophore and a 5′ quencher.'], ['3c', 'All primers were synthesized commercially by Thermofisher scientific .'], ['3d', 'The detailed information of the primers, TaqMan and HFman probes are shown in Supplementary Tables S1 and S2.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3385458a0ac4eb0be54260e74f97a7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 1/1 [00:20<00:00, 20.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all records >>> 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding data to DB: 100%|██████████| 1/1 [00:00<00:00, 459.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping duplicate document: [0a] Title: Eme...\n",
            "Skipping duplicate document: [0a] Chen et al...\n",
            "Skipping duplicate document: [0b] reported t...\n",
            "Skipping duplicate document: [0c] In our stu...\n",
            "Skipping duplicate document: [0d] Three of t...\n",
            "Skipping duplicate document: [0e] Our result...\n",
            "Skipping duplicate document: [0a] Title: Gen...\n",
            "Skipping duplicate document: [0a] The Morpho...\n",
            "Skipping duplicate document: [1b] Viral appl...\n",
            "Skipping duplicate document: [2c] Antigen-sp...\n",
            "Skipping duplicate document: [2d] To...\n",
            "Skipping duplicate document: [0a] Title: MER...\n",
            "Skipping duplicate document: [0a] These reli...\n",
            "Skipping duplicate document: [0b] No sign of...\n",
            "Skipping duplicate document: [0c] Nor did IF...\n",
            "Skipping duplicate document: [0d] Of 226 sla...\n",
            "Skipping duplicate document: [0a] Title: Kin...\n",
            "Skipping duplicate document: [0a] The cells ...\n",
            "Skipping duplicate document: [0b] The tumors...\n",
            "Skipping duplicate document: [0c] The cells ...\n",
            "Skipping duplicate document: [0d] The live a...\n",
            "Skipping duplicate document: [1e] The cells ...\n",
            "Skipping duplicate document: [1f] The differ...\n",
            "Skipping duplicate document: [0a] Title: Pan...\n",
            "Skipping duplicate document: [0a] Cumulative...\n",
            "Skipping duplicate document: [1b] In fact, t...\n",
            "Skipping duplicate document: [2c] pH1N1/2009...\n",
            "Skipping duplicate document: [2d] Contrary t...\n",
            "Skipping duplicate document: [0a] Title: Com...\n",
            "Skipping duplicate document: [0a] Illumina r...\n",
            "Skipping duplicate document: [1b] The WT seq...\n",
            "Skipping duplicate document: [1c] Sequencing...\n",
            "Skipping duplicate document: [1d] Sequenced ...\n",
            "Skipping duplicate document: [1e] Among all ...\n",
            "Skipping duplicate document: [2f] Distance b...\n",
            "Skipping duplicate document: [2g] Maximum li...\n",
            "Skipping duplicate document: [0a] Title: Asy...\n",
            "Skipping duplicate document: [0a] As spatial...\n",
            "Skipping duplicate document: [0b] This work ...\n",
            "Skipping duplicate document: [0c] Furthermor...\n",
            "Skipping duplicate document: [2d] This topic...\n",
            "Skipping duplicate document: [0a] Title: Neu...\n",
            "Skipping duplicate document: [0a] Passage: T...\n",
            "Skipping duplicate document: [0b] Although t...\n",
            "Skipping duplicate document: [0c] SARS-S pro...\n",
            "Skipping duplicate document: [0d] In many Co...\n",
            "Skipping duplicate document: [0a] Title: Tra...\n",
            "Skipping duplicate document: [0a] About half...\n",
            "Skipping duplicate document: [0b] The median...\n",
            "Skipping duplicate document: [0c] About a th...\n",
            "Skipping duplicate document: [0d] The median...\n",
            "Skipping duplicate document: [0a] Title: Glo...\n",
            "Skipping duplicate document: [0a] All primer...\n",
            "Skipping duplicate document: [0b] The sensit...\n",
            "Skipping duplicate document: [0c] Each of th...\n",
            "Skipping duplicate document: [1d] A 50-bp DN...\n",
            "Skipping duplicate document: [1e] For analys...\n",
            "total data inserted into ragbench_collection_covidqa_v16 iteration 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'default_collection' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ae21ddabfc3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"total data inserted into {milvus_collection_name} iteration {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"records : {datastor.has_entities(milvus_collection_name)} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b61a87c9859c>\u001b[0m in \u001b[0;36mhas_entities\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Collection '{name}' does not exists.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mcollection_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row_count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Retrieve the number of entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'default_collection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7peuqwYlFok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0_SylVhlMx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Placeholder function to get response from your LLM/RAG pipeline\n",
        "def get_response(query, model_name):\n",
        "    response = f\"Response from {model_name} for query: {query}\"  # Replace with actual model call\n",
        "    return response\n",
        "\n",
        "# Available LLM models\n",
        "llm_models = [\"Llama-3.2-1B\", \"GPT-4\", \"Mistral\", \"Custom-LLM\"]  # Modify as needed\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## LLM/RAG Query Interface\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Enter your query\")\n",
        "        model_selector = gr.Dropdown(llm_models, label=\"Select LLM Model\")\n",
        "\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    response_output = gr.Textbox(label=\"Response\", interactive=False)\n",
        "\n",
        "    submit_btn.click(get_response, inputs=[query_input, model_selector], outputs=response_output)\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "53IKsl8l6ejg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}